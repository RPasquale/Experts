{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LORALayer()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implementing the LORA adaptation\n",
    "        lora_adjustment = self.alpha * (x @ self.A) @ self.B\n",
    "        return nn.functional.linear(x, self.weight + lora_adjustment, self.bias)\n",
    "\n",
    "# Example usage\n",
    "input_dim = 512\n",
    "output_dim = 512\n",
    "rank = 16  # Rank for the low-rank matrices A and B\n",
    "alpha = 2  # Scaling factor for LORA adjustment\n",
    "\n",
    "lora_layer = LORALayer(input_dim, output_dim, rank, alpha)\n",
    "lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape test passed for LORALayer.\n",
      "Weights update test passed for LORALayer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the LORALayer\n",
    "input_dim, output_dim, rank, alpha = 512, 512, 16, 2\n",
    "lora_layer = LORALayer(input_dim, output_dim, rank, alpha)\n",
    "\n",
    "# Initialize an optimizer, using SGD for simplicity\n",
    "optimizer = torch.optim.SGD(lora_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(1, input_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "\n",
    "# Assert the output shape is as expected\n",
    "assert output.shape == (1, output_dim), \"Output shape is incorrect\"\n",
    "print(\"Output shape test passed for LORALayer.\")\n",
    "\n",
    "# Compute a simple loss (sum of the output)\n",
    "loss = output.sum()\n",
    "\n",
    "# Reset gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Perform backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Save the initial state of weights for comparison\n",
    "original_weight = lora_layer.weight.data.clone()\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Check if weights have been updated\n",
    "assert not torch.equal(original_weight, lora_layer.weight.data), \"Weights did not update after optimization step\"\n",
    "print(\"Weights update test passed for LORALayer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QLORALayer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize A and B\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute the LORA adjustment with quantized parameters\n",
    "        lora_adjustment = self.alpha * (x @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        return nn.functional.linear(x, self.weight + lora_adjustment, self.bias)\n",
    "\n",
    "# Example usage\n",
    "input_dim = 512\n",
    "output_dim = 512\n",
    "rank = 16\n",
    "alpha = 2\n",
    "quantization_bits = 8\n",
    "\n",
    "qlora_layer = QLORALayer(input_dim, output_dim, rank, alpha, quantization_bits)\n",
    "qlora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape test passed for QLORALayer.\n",
      "Weights update test passed for QLORALayer.\n",
      "Quantization effectiveness test passed for QLORALayer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the QLORALayer\n",
    "input_dim, output_dim, rank, alpha, quantization_bits = 512, 512, 16, 2, 8\n",
    "qlora_layer = QLORALayer(input_dim, output_dim, rank, alpha, quantization_bits)\n",
    "\n",
    "# Initialize an optimizer, using SGD for simplicity\n",
    "optimizer = torch.optim.SGD(qlora_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(1, input_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = qlora_layer(x)\n",
    "\n",
    "# Assert the output shape is as expected\n",
    "assert output.shape == (1, output_dim), \"Output shape is incorrect\"\n",
    "print(\"Output shape test passed for QLORALayer.\")\n",
    "\n",
    "# Compute a simple loss (sum of the output)\n",
    "loss = output.sum()\n",
    "\n",
    "# Reset gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Perform backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Save the initial state of weights for comparison\n",
    "original_weight = qlora_layer.weight.data.clone()\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Check if weights have been updated\n",
    "assert not torch.equal(original_weight, qlora_layer.weight.data), \"Weights did not update after optimization step\"\n",
    "print(\"Weights update test passed for QLORALayer.\")\n",
    "\n",
    "# Test for quantization effectiveness\n",
    "\n",
    "# Create copies of A and B before quantization\n",
    "A_original = qlora_layer.A.data.clone()\n",
    "B_original = qlora_layer.B.data.clone()\n",
    "\n",
    "# Perform a forward pass to apply quantization\n",
    "output = qlora_layer(x)\n",
    "\n",
    "# Retrieve the quantized versions of A and B from the forward pass\n",
    "A_quantized, _ = qlora_layer.quantize(A_original, qlora_layer.quantization_bits)\n",
    "B_quantized, _ = qlora_layer.quantize(B_original, qlora_layer.quantization_bits)\n",
    "\n",
    "# Calculate the number of unique values before and after quantization\n",
    "unique_values_A_before = torch.unique(A_original).numel()\n",
    "unique_values_B_before = torch.unique(B_original).numel()\n",
    "unique_values_A_after = torch.unique(A_quantized).numel()\n",
    "unique_values_B_after = torch.unique(B_quantized).numel()\n",
    "\n",
    "# Check if quantization reduced the number of unique values\n",
    "assert unique_values_A_after < unique_values_A_before, \"Quantization not effective on A\"\n",
    "assert unique_values_B_after < unique_values_B_before, \"Quantization not effective on B\"\n",
    "print(\"Quantization effectiveness test passed for QLORALayer.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
