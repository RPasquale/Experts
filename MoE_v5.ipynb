{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path, tokenizer_name_or_path=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "        \n",
    "        # Use a specific tokenizer if provided, else default to the model's tokenizer\n",
    "        if tokenizer_name_or_path:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        \n",
    "        # Some models may not have a pad token, set it if it's the case\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def tokenize_function(self, text):\n",
    "        # Tokenize the text input for the specific expert model\n",
    "        return self.tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Assuming input_ids are already tokenized and in the expected format for the model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits  # or return the hidden states or any other representation\n",
    "\n",
    "\n",
    "class TransformerWithMoE(nn.Module):\n",
    "    def __init__(self, experts, input_dim, num_experts, top_k, hidden_size, capacity_factor=1.0, alpha=1e-2):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.router = nn.Parameter(torch.randn(hidden_size, num_experts))\n",
    "        self.top_k = top_k\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=4)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.alpha = alpha\n",
    "        self.logits_to_hidden = nn.Linear(50257, hidden_size)  # New layer to transform logits to hidden size\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Convert input token IDs to embeddings using the first expert model\n",
    "        # Assuming all expert models have the same embedding layer\n",
    "        embeddings = self.experts[0].model.transformer.wte(input_ids)\n",
    "        print(f\"embeddings {embeddings.shape}\")\n",
    "\n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.self_attn(embeddings, embeddings, embeddings)\n",
    "        print(f\"attn_output {attn_output.shape}\")\n",
    "\n",
    "        x = self.norm1(embeddings + attn_output)\n",
    "        print(f\"x post norm1 {x.shape}\")\n",
    "      \n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        print(f\"Second attn_output {attn_output.shape}\")\n",
    "        \n",
    "        # Add & Normalize (first residual connection)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        print(f\"x post second norm1 {x.shape}\")\n",
    "\n",
    "        # Compute logits h(x) for the router\n",
    "        print(f\"Router shape: {self.router.shape}\")\n",
    "        \n",
    "        # Compute logits h(x) for the router\n",
    "        logits = x @ self.router\n",
    "        print(f\"logits shape: {logits.shape}\")\n",
    "\n",
    "        # Apply softmax to get gate values p_i(x)\n",
    "        gate_values = F.softmax(logits, dim=-1)\n",
    "        print(f\"gate_values shape: {gate_values.shape}\")\n",
    "        \n",
    "        # Get top-k gate values and indices\n",
    "        topk_gate_values, topk_indices = torch.topk(gate_values, self.top_k, dim=-1)\n",
    "        print(f\"topk_gate_values {topk_gate_values.shape}\")\n",
    "        print(f\"topk_indices {topk_indices.shape}\")\n",
    "        \n",
    "        # Initialize an empty tensor for the output\n",
    "        output = torch.zeros_like(embeddings)\n",
    "\n",
    "        # Loop over the top-k experts for each item in the batch\n",
    "        for i in range(self.top_k):\n",
    "            expert_outputs = []\n",
    "            gate_values = topk_gate_values[:, :, i]\n",
    "\n",
    "            for expert_idx in range(len(self.experts)):\n",
    "                mask = topk_indices[:, :, i] == expert_idx\n",
    "                input_masked = input_ids[mask]\n",
    "                gate_values_masked = gate_values[mask]\n",
    "\n",
    "                if input_masked.size(0) > 0:\n",
    "                    expert = self.experts[expert_idx]\n",
    "                    expert_output = expert(input_masked)\n",
    "                    expert_output = self.logits_to_hidden(expert_output)  # Transform logits to hidden size\n",
    "\n",
    "                    # Reshape to match the original batch size and sequence length\n",
    "                    reshaped_output = torch.zeros_like(embeddings)\n",
    "                    reshaped_output[mask] = expert_output\n",
    "\n",
    "                    # Prepare expanded_gate_values tensor\n",
    "                    expanded_gate_values = torch.zeros_like(embeddings)\n",
    "                    expanded_gate_values[mask] = gate_values_masked.view(-1, 1).expand(-1, embeddings.size(-1))\n",
    "                    expert_outputs.append(reshaped_output * expanded_gate_values)\n",
    "\n",
    "            # Combine the expert outputs\n",
    "            if expert_outputs:\n",
    "                expert_contributions = sum(expert_outputs)\n",
    "                output += expert_contributions\n",
    "\n",
    "\n",
    "        # Normalize the final output\n",
    "        output = self.norm2(output)\n",
    "        \n",
    "        # Calculate load balancing loss\n",
    "        f_vector = torch.zeros(self.top_k, device=output.device)\n",
    "        p_vector = torch.zeros(self.top_k, device=output.device)\n",
    "        for i in range(self.top_k):\n",
    "            f_vector[i] = torch.sum(topk_indices == i) / input_ids.size(0)\n",
    "            p_vector[i] = torch.sum(topk_gate_values[:, i])\n",
    "        \n",
    "        loss = self.alpha * self.top_k * torch.sum(f_vector * p_vector)\n",
    "        \n",
    "        return output, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate your models\n",
    "expert_gpt2 = ExpertModel(\"./gpt2-codesearchnet-dpo-py-js\", \"gpt2-medium\").to(device)\n",
    "expert_llama = ExpertModel(\"gpt2\", \"gpt2-medium\").to(device)\n",
    "experts = [expert_gpt2, expert_llama]\n",
    "\n",
    "# Correct the input dimension\n",
    "input_dim = 32  # This should be the hidden size\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "hidden_size = 768\n",
    "\n",
    "model_with_moe = TransformerWithMoE(\n",
    "    experts=experts,\n",
    "    input_dim=hidden_size,\n",
    "    num_experts=len(experts),\n",
    "    top_k=2,\n",
    "    hidden_size=hidden_size,\n",
    "    capacity_factor=1.0,\n",
    "    alpha=1e-2\n",
    ").to(device)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"teknium/GPT4-LLM-Cleaned\")\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    concatenated_texts = [instr + \" [SEP] \" + inp for instr, inp in zip(examples['instruction'], examples['input'])]\n",
    "    targets = examples['output']\n",
    "    \n",
    "    model_inputs = tokenizer(concatenated_texts, padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Instantiate your ExpertModel\n",
    "expert_gpt2 = ExpertModel(\"./gpt2-codesearchnet-dpo-py-js\", \"gpt2-medium\").to(device)\n",
    "\n",
    "# Apply the tokenize function to the dataset using the tokenizer from expert_gpt2\n",
    "tokenized_datasets = dataset.map(lambda examples: tokenize_function(examples, tokenizer=expert_gpt2.tokenizer), batched=True)\n",
    "\n",
    "# Format the dataset to output only the necessary columns for training\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Optionally, select a subset for training\n",
    "train_dataset = tokenized_datasets[\"train\"].select(range(0, 2000)) \n",
    "\n",
    "# Create dataloader from train dataset\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training or evaluation loop\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    # Forward pass through your model\n",
    "    outputs, loss = model_with_moe(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4299a35c717e414883cf7dcb7e3990db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n",
      "embeddings torch.Size([1, 512, 768])\n",
      "attn_output torch.Size([1, 512, 768])\n",
      "x post norm1 torch.Size([1, 512, 768])\n",
      "Second attn_output torch.Size([1, 512, 768])\n",
      "x post second norm1 torch.Size([1, 512, 768])\n",
      "Router shape: torch.Size([768, 2])\n",
      "logits shape: torch.Size([1, 512, 2])\n",
      "gate_values shape: torch.Size([1, 512, 2])\n",
      "topk_gate_values torch.Size([1, 512, 2])\n",
      "topk_indices torch.Size([1, 512, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 189\u001b[0m\n\u001b[0;32m    186\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Forward pass through your model\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_with_moe\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 96\u001b[0m, in \u001b[0;36mTransformerWithMoE.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     94\u001b[0m expert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts[expert_idx]\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#expert_output = expert(input_masked)\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m expert_output \u001b[38;5;241m=\u001b[39m \u001b[43mexpert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m expert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits_to_hidden(expert_output)  \u001b[38;5;66;03m# Transform logits to hidden size\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Reshape to match the original batch size and sequence length\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mExpertModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Assuming input_ids are already tokenized and in the expected format for the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    878\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    885\u001b[0m         output_attentions,\n\u001b[0;32m    886\u001b[0m     )\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:208\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m--> 208\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[0;32m    211\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mtype(value\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:1826\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1822\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 1826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \n\u001b[0;32m   1829\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m \n\u001b[0;32m   1850\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path, tokenizer_name_or_path=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "        \n",
    "        # Use a specific tokenizer if provided, else default to the model's tokenizer\n",
    "        if tokenizer_name_or_path:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        \n",
    "        # Some models may not have a pad token, set it if it's the case\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def tokenize_function(self, text):\n",
    "        # Tokenize the text input for the specific expert model\n",
    "        return self.tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Assuming input_ids are already tokenized and in the expected format for the model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits  # or return the hidden states or any other representation\n",
    "\n",
    "\n",
    "class TransformerWithMoE(nn.Module):\n",
    "    def __init__(self, experts, input_dim, num_experts, top_k, hidden_size, capacity_factor=1.0, alpha=1e-2):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.router = nn.Parameter(torch.randn(hidden_size, num_experts))\n",
    "        self.top_k = top_k\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=4)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.alpha = alpha\n",
    "        self.logits_to_hidden = nn.Linear(50257, hidden_size)  # New layer to transform logits to hidden size\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Convert input token IDs to embeddings using the first expert model\n",
    "        # Assuming all expert models have the same embedding layer\n",
    "        embeddings = self.experts[0].model.transformer.wte(input_ids)\n",
    "        print(f\"embeddings {embeddings.shape}\")\n",
    "\n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.self_attn(embeddings, embeddings, embeddings)\n",
    "        print(f\"attn_output {attn_output.shape}\")\n",
    "\n",
    "        x = self.norm1(embeddings + attn_output)\n",
    "        print(f\"x post norm1 {x.shape}\")\n",
    "      \n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        print(f\"Second attn_output {attn_output.shape}\")\n",
    "        \n",
    "        # Add & Normalize (first residual connection)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        print(f\"x post second norm1 {x.shape}\")\n",
    "\n",
    "        # Compute logits h(x) for the router\n",
    "        print(f\"Router shape: {self.router.shape}\")\n",
    "        \n",
    "        # Compute logits h(x) for the router\n",
    "        logits = x @ self.router\n",
    "        print(f\"logits shape: {logits.shape}\")\n",
    "\n",
    "        # Apply softmax to get gate values p_i(x)\n",
    "        gate_values = F.softmax(logits, dim=-1)\n",
    "        print(f\"gate_values shape: {gate_values.shape}\")\n",
    "        \n",
    "        # Get top-k gate values and indices\n",
    "        topk_gate_values, topk_indices = torch.topk(gate_values, self.top_k, dim=-1)\n",
    "        print(f\"topk_gate_values {topk_gate_values.shape}\")\n",
    "        print(f\"topk_indices {topk_indices.shape}\")\n",
    "        \n",
    "        # Initialize an empty tensor for the output\n",
    "        output = torch.zeros_like(embeddings)\n",
    "\n",
    "        # Loop over the top-k experts for each item in the batch\n",
    "        for i in range(self.top_k):\n",
    "            expert_outputs = []\n",
    "            gate_values = topk_gate_values[:, :, i]\n",
    "\n",
    "            for expert_idx in range(len(self.experts)):\n",
    "                mask = topk_indices[:, :, i] == expert_idx\n",
    "                input_masked = input_ids[mask]\n",
    "                gate_values_masked = gate_values[mask]\n",
    "\n",
    "                if input_masked.size(0) > 0:\n",
    "                    expert = self.experts[expert_idx]\n",
    "                    #expert_output = expert(input_masked)\n",
    "                    expert_output = expert(input_masked, attention_mask=attention_mask[mask] if attention_mask is not None else None)\n",
    "\n",
    "                    expert_output = self.logits_to_hidden(expert_output)  # Transform logits to hidden size\n",
    "\n",
    "                    # Reshape to match the original batch size and sequence length\n",
    "                    reshaped_output = torch.zeros_like(embeddings)\n",
    "                    reshaped_output[mask] = expert_output\n",
    "\n",
    "                    # Prepare expanded_gate_values tensor\n",
    "                    expanded_gate_values = torch.zeros_like(embeddings)\n",
    "                    expanded_gate_values[mask] = gate_values_masked.view(-1, 1).expand(-1, embeddings.size(-1))\n",
    "                    expert_outputs.append(reshaped_output * expanded_gate_values)\n",
    "\n",
    "            # Combine the expert outputs\n",
    "            if expert_outputs:\n",
    "                expert_contributions = sum(expert_outputs)\n",
    "                output += expert_contributions\n",
    "\n",
    "\n",
    "        # Normalize the final output\n",
    "        output = self.norm2(output)\n",
    "        \n",
    "        # Calculate load balancing loss\n",
    "        f_vector = torch.zeros(self.top_k, device=output.device)\n",
    "        p_vector = torch.zeros(self.top_k, device=output.device)\n",
    "        for i in range(self.top_k):\n",
    "            f_vector[i] = torch.sum(topk_indices == i) / input_ids.size(0)\n",
    "            p_vector[i] = torch.sum(topk_gate_values[:, i])\n",
    "        \n",
    "        loss = self.alpha * self.top_k * torch.sum(f_vector * p_vector)\n",
    "        \n",
    "        return output, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate your models\n",
    "expert_gpt2 = ExpertModel(\"./gpt2-codesearchnet-dpo-py-js\", \"gpt2-medium\").to(device)\n",
    "expert_llama = ExpertModel(\"gpt2\", \"gpt2-medium\").to(device)\n",
    "experts = [expert_gpt2, expert_llama]\n",
    "\n",
    "# Correct the input dimension\n",
    "input_dim = 32  # This should be the hidden size\n",
    "batch_size = 1\n",
    "seq_length = 32\n",
    "hidden_size = 768\n",
    "\n",
    "model_with_moe = TransformerWithMoE(\n",
    "    experts=experts,\n",
    "    input_dim=hidden_size,\n",
    "    num_experts=len(experts),\n",
    "    top_k=2,\n",
    "    hidden_size=hidden_size,\n",
    "    capacity_factor=1.0,\n",
    "    alpha=1e-2\n",
    ").to(device)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"teknium/GPT4-LLM-Cleaned\")\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    concatenated_texts = [instr + \" [SEP] \" + inp for instr, inp in zip(examples['instruction'], examples['input'])]\n",
    "    targets = examples['output']\n",
    "    \n",
    "    model_inputs = tokenizer(concatenated_texts, padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Instantiate your ExpertModel\n",
    "expert_gpt2 = ExpertModel(\"./gpt2-codesearchnet-dpo-py-js\", \"gpt2-medium\").to(device)\n",
    "\n",
    "# Apply the tokenize function to the dataset using the tokenizer from expert_gpt2\n",
    "tokenized_datasets = dataset.map(lambda examples: tokenize_function(examples, tokenizer=expert_gpt2.tokenizer), batched=True)\n",
    "\n",
    "# Format the dataset to output only the necessary columns for training\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Optionally, select a subset for training\n",
    "train_dataset = tokenized_datasets[\"train\"].select(range(0, 100)) \n",
    "\n",
    "# Create dataloader from train dataset\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training or evaluation loop\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    # Forward pass through your model\n",
    "    outputs, loss = model_with_moe(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
