{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (4.36.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: pip in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (23.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install -U -q transformers\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q datasets\n",
    "!pip install -q -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # Check if CUDA is available\n",
    "print(torch.cuda.get_device_name(0))  # Get the GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Expert 1- Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc5bada7c074431b1e0a2ca7b6520b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.271, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 1.2574, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 1.2408, 'learning_rate': 4.8e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2417, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}\n",
      "{'loss': 1.2925, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3231, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 1.301, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2404, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}\n",
      "{'loss': 1.148, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3483, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2259, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2077, 'learning_rate': 4.2e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2801, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.52}\n",
      "{'loss': 1.281, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2697, 'learning_rate': 4e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3084, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}\n",
      "{'loss': 1.2095, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1981, 'learning_rate': 3.8e-05, 'epoch': 0.72}\n",
      "{'loss': 1.295, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1819, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2251, 'learning_rate': 3.6e-05, 'epoch': 0.84}\n",
      "{'loss': 1.2176, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.88}\n",
      "{'loss': 1.3216, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.92}\n",
      "{'loss': 1.235, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./gpt2-codesearchnet\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.201, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0745, 'learning_rate': 3.266666666666667e-05, 'epoch': 1.04}\n",
      "{'loss': 1.1324, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.08}\n",
      "{'loss': 1.1152, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}\n",
      "{'loss': 1.1383, 'learning_rate': 3.066666666666667e-05, 'epoch': 1.16}\n",
      "{'loss': 1.0826, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 1.1426, 'learning_rate': 2.9333333333333336e-05, 'epoch': 1.24}\n",
      "{'loss': 1.0812, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 1.0586, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.32}\n",
      "{'loss': 1.1316, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.36}\n",
      "{'loss': 1.1482, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n",
      "{'loss': 1.1202, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}\n",
      "{'loss': 1.1005, 'learning_rate': 2.5333333333333337e-05, 'epoch': 1.48}\n",
      "{'loss': 1.0816, 'learning_rate': 2.466666666666667e-05, 'epoch': 1.52}\n",
      "{'loss': 1.0772, 'learning_rate': 2.4e-05, 'epoch': 1.56}\n",
      "{'loss': 1.146, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n",
      "{'loss': 1.1494, 'learning_rate': 2.2666666666666668e-05, 'epoch': 1.64}\n",
      "{'loss': 1.1627, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.68}\n",
      "{'loss': 1.0766, 'learning_rate': 2.1333333333333335e-05, 'epoch': 1.72}\n",
      "{'loss': 1.0848, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}\n",
      "{'loss': 1.167, 'learning_rate': 2e-05, 'epoch': 1.8}\n",
      "{'loss': 1.0888, 'learning_rate': 1.9333333333333333e-05, 'epoch': 1.84}\n",
      "{'loss': 1.0666, 'learning_rate': 1.866666666666667e-05, 'epoch': 1.88}\n",
      "{'loss': 1.1528, 'learning_rate': 1.8e-05, 'epoch': 1.92}\n",
      "{'loss': 1.0989, 'learning_rate': 1.7333333333333336e-05, 'epoch': 1.96}\n",
      "{'loss': 1.1559, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'loss': 1.0382, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.04}\n",
      "{'loss': 1.0057, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}\n",
      "{'loss': 1.0631, 'learning_rate': 1.4666666666666668e-05, 'epoch': 2.12}\n",
      "{'loss': 1.0794, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.16}\n",
      "{'loss': 1.0586, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n",
      "{'loss': 1.1456, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}\n",
      "{'loss': 1.041, 'learning_rate': 1.2e-05, 'epoch': 2.28}\n",
      "{'loss': 1.0835, 'learning_rate': 1.1333333333333334e-05, 'epoch': 2.32}\n",
      "{'loss': 1.0276, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.36}\n",
      "{'loss': 1.0748, 'learning_rate': 1e-05, 'epoch': 2.4}\n",
      "{'loss': 1.0203, 'learning_rate': 9.333333333333334e-06, 'epoch': 2.44}\n",
      "{'loss': 1.0189, 'learning_rate': 8.666666666666668e-06, 'epoch': 2.48}\n",
      "{'loss': 1.0527, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.52}\n",
      "{'loss': 1.1045, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}\n",
      "{'loss': 1.0494, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n",
      "{'loss': 1.0973, 'learning_rate': 6e-06, 'epoch': 2.64}\n",
      "{'loss': 1.1186, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.68}\n",
      "{'loss': 1.0591, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}\n",
      "{'loss': 1.0198, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.76}\n",
      "{'loss': 1.0569, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n",
      "{'loss': 0.9633, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.84}\n",
      "{'loss': 0.9998, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}\n",
      "{'loss': 1.0531, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.92}\n",
      "{'loss': 1.0105, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.96}\n",
      "{'loss': 0.9947, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 5144.7607, 'train_samples_per_second': 1.166, 'train_steps_per_second': 0.292, 'train_loss': 1.1385620231628417, 'epoch': 3.0}\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4648e95bc984f11ba69550d43dda71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ab7c21d3154380a2ec06dc5ea98121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7a250e0f4f47f0a5d79a477944270a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Checkpoint destination directory ./model_output\\checkpoint-10 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6849, 'learning_rate': 3e-06, 'rewards/chosen': -0.03544330596923828, 'rewards/rejected': -0.056198056787252426, 'rewards/accuracies': 0.25833332538604736, 'rewards/margins': 0.020754750818014145, 'logps/rejected': -393.04083251953125, 'logps/chosen': -383.63580322265625, 'logits/rejected': -27.472835540771484, 'logits/chosen': -27.7117862701416, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-20 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6236, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': -0.1736384481191635, 'rewards/rejected': -0.5431264042854309, 'rewards/accuracies': 0.6583333611488342, 'rewards/margins': 0.3694879710674286, 'logps/rejected': -367.2397766113281, 'logps/chosen': -368.5068359375, 'logits/rejected': -27.445049285888672, 'logits/chosen': -27.823726654052734, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-30 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6734, 'learning_rate': 1.25e-05, 'rewards/chosen': -1.2601524591445923, 'rewards/rejected': -2.220651149749756, 'rewards/accuracies': 0.6833333373069763, 'rewards/margins': 0.9604988694190979, 'logps/rejected': -403.0313720703125, 'logps/chosen': -394.6908264160156, 'logits/rejected': -27.852750778198242, 'logits/chosen': -27.752178192138672, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-40 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6259, 'learning_rate': 1.75e-05, 'rewards/chosen': -0.8907012939453125, 'rewards/rejected': -3.220024585723877, 'rewards/accuracies': 0.699999988079071, 'rewards/margins': 2.3293235301971436, 'logps/rejected': -411.392578125, 'logps/chosen': -407.0198669433594, 'logits/rejected': -28.221221923828125, 'logits/chosen': -28.57944107055664, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8121, 'learning_rate': 2.25e-05, 'rewards/chosen': 0.4933929443359375, 'rewards/rejected': -2.754119873046875, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 3.2475128173828125, 'logps/rejected': -419.8365173339844, 'logps/chosen': -401.4595947265625, 'logits/rejected': -28.402685165405273, 'logits/chosen': -27.50358772277832, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-60 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1894, 'learning_rate': 2.7500000000000004e-05, 'rewards/chosen': 1.541765570640564, 'rewards/rejected': -2.8910584449768066, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 4.43282413482666, 'logps/rejected': -398.05450439453125, 'logps/chosen': -393.93072509765625, 'logits/rejected': -26.181493759155273, 'logits/chosen': -26.767601013183594, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-70 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3071, 'learning_rate': 3.2500000000000004e-05, 'rewards/chosen': 0.37925484776496887, 'rewards/rejected': -4.146799564361572, 'rewards/accuracies': 0.7666666507720947, 'rewards/margins': 4.526054382324219, 'logps/rejected': -370.5465393066406, 'logps/chosen': -410.6764221191406, 'logits/rejected': -27.533910751342773, 'logits/chosen': -28.77449607849121, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-80 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7924, 'learning_rate': 3.7500000000000003e-05, 'rewards/chosen': 2.93487286567688, 'rewards/rejected': -3.9991438388824463, 'rewards/accuracies': 0.6916666626930237, 'rewards/margins': 6.934017658233643, 'logps/rejected': -362.28466796875, 'logps/chosen': -391.1781921386719, 'logits/rejected': -26.513166427612305, 'logits/chosen': -27.775205612182617, 'epoch': 0.96}\n",
      "{'train_runtime': 586.0087, 'train_samples_per_second': 1.706, 'train_steps_per_second': 0.142, 'train_loss': 1.0003162809165127, 'epoch': 1.0}\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9181a7f6145c4120b198f06e6abef0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4792, 'learning_rate': 4.9401197604790424e-05, 'epoch': 0.04}\n",
      "{'loss': 1.4541, 'learning_rate': 4.873586161011311e-05, 'epoch': 0.08}\n",
      "{'loss': 1.3566, 'learning_rate': 4.80705256154358e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3989, 'learning_rate': 4.7405189620758485e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4902, 'learning_rate': 4.673985362608118e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3003, 'learning_rate': 4.6074517631403865e-05, 'epoch': 0.24}\n",
      "{'loss': 1.3627, 'learning_rate': 4.540918163672655e-05, 'epoch': 0.28}\n",
      "{'loss': 1.4296, 'learning_rate': 4.474384564204924e-05, 'epoch': 0.32}\n",
      "{'loss': 1.417, 'learning_rate': 4.4078509647371926e-05, 'epoch': 0.36}\n",
      "{'loss': 1.3579, 'learning_rate': 4.341317365269461e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3682, 'learning_rate': 4.274783765801731e-05, 'epoch': 0.44}\n",
      "{'loss': 1.3835, 'learning_rate': 4.2082501663339994e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3177, 'learning_rate': 4.141716566866268e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2993, 'learning_rate': 4.075182967398537e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2917, 'learning_rate': 4.0086493679308054e-05, 'epoch': 0.6}\n",
      "{'loss': 1.341, 'learning_rate': 3.942115768463074e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3918, 'learning_rate': 3.875582168995343e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3085, 'learning_rate': 3.8090485695276115e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2806, 'learning_rate': 3.74251497005988e-05, 'epoch': 0.76}\n",
      "{'loss': 1.3259, 'learning_rate': 3.675981370592149e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2547, 'learning_rate': 3.6094477711244176e-05, 'epoch': 0.84}\n",
      "{'loss': 1.3333, 'learning_rate': 3.542914171656686e-05, 'epoch': 0.88}\n",
      "{'loss': 1.3691, 'learning_rate': 3.4763805721889556e-05, 'epoch': 0.92}\n",
      "{'loss': 1.3904, 'learning_rate': 3.409846972721224e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./gpt2-codesearchnet-finetuned-dpo-args\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.275, 'learning_rate': 3.343313373253493e-05, 'epoch': 1.0}\n",
      "{'loss': 1.2533, 'learning_rate': 3.276779773785762e-05, 'epoch': 1.04}\n",
      "{'loss': 1.1588, 'learning_rate': 3.2102461743180304e-05, 'epoch': 1.08}\n",
      "{'loss': 1.2272, 'learning_rate': 3.143712574850299e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2371, 'learning_rate': 3.0771789753825685e-05, 'epoch': 1.16}\n",
      "{'loss': 1.1376, 'learning_rate': 3.013972055888224e-05, 'epoch': 1.2}\n",
      "{'loss': 1.2122, 'learning_rate': 2.9474384564204926e-05, 'epoch': 1.24}\n",
      "{'loss': 1.1916, 'learning_rate': 2.8809048569527613e-05, 'epoch': 1.28}\n",
      "{'loss': 1.1891, 'learning_rate': 2.81437125748503e-05, 'epoch': 1.32}\n",
      "{'loss': 1.2364, 'learning_rate': 2.7478376580172987e-05, 'epoch': 1.36}\n",
      "{'loss': 1.2102, 'learning_rate': 2.6813040585495673e-05, 'epoch': 1.4}\n",
      "{'loss': 1.1813, 'learning_rate': 2.6147704590818367e-05, 'epoch': 1.44}\n",
      "{'loss': 1.1859, 'learning_rate': 2.5482368596141054e-05, 'epoch': 1.48}\n",
      "{'loss': 1.1703, 'learning_rate': 2.481703260146374e-05, 'epoch': 1.52}\n",
      "{'loss': 1.2924, 'learning_rate': 2.4151696606786428e-05, 'epoch': 1.56}\n",
      "{'loss': 1.1847, 'learning_rate': 2.3486360612109115e-05, 'epoch': 1.6}\n",
      "{'loss': 1.1671, 'learning_rate': 2.2821024617431805e-05, 'epoch': 1.64}\n",
      "{'loss': 1.1803, 'learning_rate': 2.2155688622754492e-05, 'epoch': 1.68}\n",
      "{'loss': 1.0993, 'learning_rate': 2.149035262807718e-05, 'epoch': 1.72}\n",
      "{'loss': 1.1914, 'learning_rate': 2.082501663339987e-05, 'epoch': 1.76}\n",
      "{'loss': 1.2188, 'learning_rate': 2.0159680638722556e-05, 'epoch': 1.8}\n",
      "{'loss': 1.2676, 'learning_rate': 1.9494344644045243e-05, 'epoch': 1.84}\n",
      "{'loss': 1.2751, 'learning_rate': 1.8829008649367933e-05, 'epoch': 1.88}\n",
      "{'loss': 1.1989, 'learning_rate': 1.816367265469062e-05, 'epoch': 1.92}\n",
      "{'loss': 1.2788, 'learning_rate': 1.7498336660013307e-05, 'epoch': 1.96}\n",
      "{'loss': 1.152, 'learning_rate': 1.6833000665335997e-05, 'epoch': 2.0}\n",
      "{'loss': 1.0784, 'learning_rate': 1.6167664670658684e-05, 'epoch': 2.04}\n",
      "{'loss': 1.1582, 'learning_rate': 1.550232867598137e-05, 'epoch': 2.08}\n",
      "{'loss': 1.1822, 'learning_rate': 1.483699268130406e-05, 'epoch': 2.12}\n",
      "{'loss': 1.1383, 'learning_rate': 1.4171656686626747e-05, 'epoch': 2.16}\n",
      "{'loss': 1.0231, 'learning_rate': 1.3506320691949434e-05, 'epoch': 2.2}\n",
      "{'loss': 1.0856, 'learning_rate': 1.2840984697272124e-05, 'epoch': 2.24}\n",
      "{'loss': 1.2554, 'learning_rate': 1.2175648702594811e-05, 'epoch': 2.28}\n",
      "{'loss': 1.1272, 'learning_rate': 1.15103127079175e-05, 'epoch': 2.32}\n",
      "{'loss': 1.0536, 'learning_rate': 1.0844976713240188e-05, 'epoch': 2.36}\n",
      "{'loss': 1.1378, 'learning_rate': 1.0179640718562875e-05, 'epoch': 2.4}\n",
      "{'loss': 1.1678, 'learning_rate': 9.514304723885564e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1233, 'learning_rate': 8.84896872920825e-06, 'epoch': 2.48}\n",
      "{'loss': 1.142, 'learning_rate': 8.183632734530937e-06, 'epoch': 2.51}\n",
      "{'loss': 1.1037, 'learning_rate': 7.518296739853627e-06, 'epoch': 2.55}\n",
      "{'loss': 1.203, 'learning_rate': 6.852960745176315e-06, 'epoch': 2.59}\n",
      "{'loss': 1.15, 'learning_rate': 6.187624750499002e-06, 'epoch': 2.63}\n",
      "{'loss': 1.1376, 'learning_rate': 5.52228875582169e-06, 'epoch': 2.67}\n",
      "{'loss': 1.1246, 'learning_rate': 4.856952761144378e-06, 'epoch': 2.71}\n",
      "{'loss': 1.0626, 'learning_rate': 4.191616766467066e-06, 'epoch': 2.75}\n",
      "{'loss': 1.107, 'learning_rate': 3.5262807717897543e-06, 'epoch': 2.79}\n",
      "{'loss': 1.0818, 'learning_rate': 2.860944777112442e-06, 'epoch': 2.83}\n",
      "{'loss': 1.176, 'learning_rate': 2.19560878243513e-06, 'epoch': 2.87}\n",
      "{'loss': 1.0427, 'learning_rate': 1.5302727877578178e-06, 'epoch': 2.91}\n",
      "{'loss': 1.1483, 'learning_rate': 8.649367930805056e-07, 'epoch': 2.95}\n",
      "{'loss': 1.1161, 'learning_rate': 1.996007984031936e-07, 'epoch': 2.99}\n",
      "{'train_runtime': 4136.1299, 'train_samples_per_second': 1.451, 'train_steps_per_second': 0.363, 'train_loss': 1.228739901217158, 'epoch': 3.0}\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0fea5d7a53419ebe1971e63000e526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a574a86e6ddf4338a61e37830d4c6896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4511 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Checkpoint destination directory ./model_output\\checkpoint-10 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6868, 'learning_rate': 2.5e-06, 'rewards/chosen': -0.006717236712574959, 'rewards/rejected': -0.020466836169362068, 'rewards/accuracies': 0.18333333730697632, 'rewards/margins': 0.01374959945678711, 'logps/rejected': -404.7783508300781, 'logps/chosen': -400.45892333984375, 'logits/rejected': 3.5620877742767334, 'logits/chosen': 2.15421986579895, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-20 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6131, 'learning_rate': 7.5e-06, 'rewards/chosen': -0.6784206628799438, 'rewards/rejected': -1.1542631387710571, 'rewards/accuracies': 0.625, 'rewards/margins': 0.47584250569343567, 'logps/rejected': -405.0921325683594, 'logps/chosen': -386.9043273925781, 'logits/rejected': 4.737513065338135, 'logits/chosen': 3.506503105163574, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-30 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6768, 'learning_rate': 1.25e-05, 'rewards/chosen': -0.641642689704895, 'rewards/rejected': -2.0049540996551514, 'rewards/accuracies': 0.7083333134651184, 'rewards/margins': 1.363311529159546, 'logps/rejected': -399.8486328125, 'logps/chosen': -406.9419860839844, 'logits/rejected': 2.7293238639831543, 'logits/chosen': 2.1308655738830566, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-40 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.631, 'learning_rate': 1.7000000000000003e-05, 'rewards/chosen': 0.44887658953666687, 'rewards/rejected': -1.8868328332901, 'rewards/accuracies': 0.7583333253860474, 'rewards/margins': 2.3357090950012207, 'logps/rejected': -393.0673828125, 'logps/chosen': -392.3971862792969, 'logits/rejected': -0.7178871035575867, 'logits/chosen': -1.4798275232315063, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.829, 'learning_rate': 2.2000000000000003e-05, 'rewards/chosen': 1.1804136037826538, 'rewards/rejected': -1.8838536739349365, 'rewards/accuracies': 0.6916666626930237, 'rewards/margins': 3.064267158508301, 'logps/rejected': -386.8988037109375, 'logps/chosen': -393.7730407714844, 'logits/rejected': 0.4945967495441437, 'logits/chosen': -2.4242303371429443, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-60 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1651, 'learning_rate': 2.7000000000000002e-05, 'rewards/chosen': -3.2439167499542236, 'rewards/rejected': -8.207462310791016, 'rewards/accuracies': 0.7749999761581421, 'rewards/margins': 4.9635467529296875, 'logps/rejected': -428.8787841796875, 'logps/chosen': -378.6533203125, 'logits/rejected': 0.9358721971511841, 'logits/chosen': 0.8868151307106018, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-70 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3287, 'learning_rate': 3.2000000000000005e-05, 'rewards/chosen': 0.021077314391732216, 'rewards/rejected': -5.554483890533447, 'rewards/accuracies': 0.800000011920929, 'rewards/margins': 5.575562477111816, 'logps/rejected': -399.04974365234375, 'logps/chosen': -357.0007019042969, 'logits/rejected': -0.2401006668806076, 'logits/chosen': -0.8948389887809753, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./model_output\\checkpoint-80 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0623, 'learning_rate': 3.7e-05, 'rewards/chosen': 1.2345842123031616, 'rewards/rejected': -4.46947717666626, 'rewards/accuracies': 0.699999988079071, 'rewards/margins': 5.704061508178711, 'logps/rejected': -407.26837158203125, 'logps/chosen': -394.05206298828125, 'logits/rejected': 1.1850199699401855, 'logits/chosen': -0.8237319588661194, 'epoch': 0.96}\n",
      "{'train_runtime': 713.7572, 'train_samples_per_second': 1.401, 'train_steps_per_second': 0.116, 'train_loss': 1.02782260366233, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Initialize the tokenizer and set the padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the eos_token as the pad_token\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Use a subset of the tokenized dataset for training\n",
    "#train_dataset = tokenized_dataset[\"train\"].select(range(1000))  # Adjust the range as needed\n",
    "train_dataset = tokenized_dataset[\"train\"].select(range(2001, 4001))  # Select data from indices 1001 to 2000\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-codesearchnet\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU's capability\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=20,  # Add this line to log every 20 steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# After fine-tuning\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "trainer.save_model(fine_tuned_model_path)\n",
    "\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",  # Directory where the model checkpoints will be saved.\n",
    "    num_train_epochs=1,          # Total number of training epochs.\n",
    "    per_device_train_batch_size=2,  # Batch size per device during training.\n",
    "    per_device_eval_batch_size=2,   # Batch size for evaluation.\n",
    "    gradient_accumulation_steps=6, # Accumulate gradients over two steps to save memory\n",
    "    fp16=True, \n",
    "    warmup_steps=100,             # Number of warmup steps for learning rate scheduler.\n",
    "    weight_decay=0.01,            # Strength of weight decay.\n",
    "    logging_dir='./logs',         # Directory for storing logs.\n",
    "    logging_steps=10,             # Log every X updates steps.\n",
    "    save_steps=10,              # Save checkpoint every X updates steps.\n",
    "    remove_unused_columns=False,  # Add this line,\n",
    "    # Add other parameters as needed\n",
    ")\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "def return_prompt_and_responses(samples) -> Dict[str, list]:\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],   # rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # rated worse than j\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    split=\"train\",\n",
    "    data_dir=\"data/rl\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subset_size = 1000\n",
    "start_index = 500  # Starting index for the second chunk\n",
    "end_index = start_index + subset_size  # Ending index for the second chunk\n",
    "# Select the second chunk of the dataset\n",
    "subset_indices = range(start_index, end_index)\n",
    "dataset = dataset.select(subset_indices)\n",
    "\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    beta=0.5,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 256,\n",
    "    max_prompt_length = 128\n",
    "\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "\n",
    "model_save_path_after_dpo = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "dpo_trainer.save_model(model_save_path_after_dpo)\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"javascript\")\n",
    "\n",
    "# Initialize the tokenizer and set the padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the eos_token as the pad_token\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Use a subset of the tokenized dataset for training\n",
    "#train_dataset = tokenized_dataset[\"train\"].select(range(1000))  # Adjust the range as needed\n",
    "train_dataset = tokenized_dataset[\"train\"].select(range(2000, 4001))  # Select data from indices 1001 to 2000\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-codesearchnet-finetuned-dpo-args\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU's capability\n",
    "    save_steps=500,\n",
    "    logging_steps=20\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# After fine-tuning\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "trainer.save_model(fine_tuned_model_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",  # Directory where the model checkpoints will be saved.\n",
    "    num_train_epochs=1,          # Total number of training epochs.\n",
    "    per_device_train_batch_size=2,  # Batch size per device during training.\n",
    "    per_device_eval_batch_size=2,   # Batch size for evaluation.\n",
    "    gradient_accumulation_steps=6, # Accumulate gradients over two steps to save memory\n",
    "    fp16=True, \n",
    "    warmup_steps=100,             # Number of warmup steps for learning rate scheduler.\n",
    "    weight_decay=0.01,            # Strength of weight decay.\n",
    "    logging_dir='./logs',         # Directory for storing logs.\n",
    "    logging_steps=10,             # Log every X updates steps.\n",
    "    save_steps=10,              # Save checkpoint every X updates steps.\n",
    "    remove_unused_columns=False,  # Add this line,\n",
    "    # Add other parameters as needed\n",
    ")\n",
    "fine_tuned_model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "def return_prompt_and_responses(samples) -> Dict[str, list]:\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],   # rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # rated worse than j\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    split=\"train\",\n",
    "    data_dir=\"data/rl\"\n",
    ")\n",
    "\n",
    "start_index = 0  # Starting index for the second chunk\n",
    "end_index = start_index + subset_size  # Ending index for the second chunk\n",
    "\n",
    "# Select the second chunk of the dataset\n",
    "subset_indices = range(start_index, end_index)\n",
    "dataset = dataset.select(subset_indices)\n",
    "\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    beta=0.5,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 256,\n",
    "    max_prompt_length = 128\n",
    "\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "\n",
    "model_save_path_after_dpo = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "dpo_trainer.save_model(model_save_path_after_dpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many parameters in Expert 1 Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124439808\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_parameters = count_parameters(model)\n",
    "print(f\"Total number of parameters: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Expert 1 Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_path = \"./gpt2-codesearchnet-dpo-py-js\"\n",
    "\n",
    "# Configure bitsandbytes quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant = False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=cache_dir  \n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_simple(model, tokenizer, prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with a custom prompt\n",
    "prompt = \"Write python code to add 3 and 1\"\n",
    "generated_text = generate_simple(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert 2- Preference Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(model_id)\n",
    "# config.max_position_embeddings = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #config = config,\n",
    "    #quantization_config = bnb_config,\n",
    "    #rope_scaling = {\"type\": \"Linear\",\"factor\": 2.0},\n",
    "    device_map = 'auto',\n",
    "    #trust_remote_code = False,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    #use_flash_attention_2 = True, # dep apparently\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    cache_dir = cache_dir\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The dog: (laughing)\n",
      "\n",
      "The Quick: (laughing)\n",
      "\n",
      "The dog: (laughing)\n",
      "\n",
      "The Quick: (laughing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = True)\n",
    "\n",
    "def generate_simple(model, tokenizer, prompt):\n",
    "  inputs = tokenizer.encode(prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "  outputs = model.generate(inputs, max_length= 50)\n",
    "  return tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "prompt = \"The Quick brown fox\"\n",
    "generated_text = generate_simple(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  trainable_params = 0\n",
    "  non_trainable_params = 0\n",
    "  all_params = 0\n",
    "\n",
    "  print(\"Trainable Parameters: \")\n",
    "  for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "      print(f\" {name}\")\n",
    "    else:\n",
    "      non_trainable_params += param.numel()\n",
    "\n",
    "  print(\"\\nNon-Trainable Parameters: \")\n",
    "  for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "      print(f\" {name}\")\n",
    "\n",
    "\n",
    "  print(\n",
    "      f\"\\nSummary:\\n Trainable Params: {trainable_params} \\n Non-Trainable params: {non_trainable_params}\"\n",
    "  )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('C')}\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('/matplotlib_inline.backend_inline'), WindowsPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=121, Highest Compute Capability: 8.6.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Loading binary c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[0;32m      2\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m      3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      4\u001b[0m     lora_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\mapping.py:133\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    132\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\peft_model.py:1043\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\peft_model.py:125\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:111\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:90\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:247\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m    242\u001b[0m     optional_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[0;32m    246\u001b[0m     }\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:168\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[1;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m linear_types \u001b[38;5;241m=\u001b[39m (Linear,)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear8bitLt\n\u001b[0;32m    170\u001b[0m     linear_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (Linear8bitLt,)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_4bit_available():\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     MatmulLtState,\n\u001b[0;32m      9\u001b[0m     bmm_cublas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     matmul_4bit\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     switchback_bnb,\n\u001b[0;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[0;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[0;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\bitsandbytes\\cextension.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mgenerate_instructions()\n\u001b[0;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[0;32m     29\u001b[0m lib\u001b[38;5;241m.\u001b[39mget_context\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mc_void_p\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"self_attn.q_proj\",\n",
    "                      \"self_attn.k_proj\",\n",
    "                      \"self_attn.v_proj\",\n",
    "                      \"self_attn.o_proj\",\n",
    "                      #\"self_attn.rotary_emb.inv_freq\",\n",
    "                      \"mlp.gate_proj\",\n",
    "                      \"mlp.up_proj\",\n",
    "                      \"mlp.down_proj\",\n",
    "                      ],\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v0.6', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast= True)\n",
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer EOS token ID:  2\n",
      "Tokenizer EOS token:  </s>\n",
      "Model EOS token ID:  2\n",
      "Model EOS token:  </s>\n",
      "Model BOS token ID:  2\n",
      "Model BOS token:  </s>\n",
      "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v0.6', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "  added_tokens = tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "else:\n",
    "  added_tokens = 0\n",
    "\n",
    "\n",
    "if added_tokens > 0:\n",
    "  model.resize_token_embeddings(len(tokenizer))\n",
    "  print('\\n\\nResizing token embeddings for the model\\n\\n')\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token is wrong length\"\n",
    "assert model.config.eos_token_id == tokenizer.eos_token_id, \"The model's pad token is wrong length\"\n",
    "\n",
    "print('Tokenizer EOS token ID: ', tokenizer.eos_token_id)\n",
    "print('Tokenizer EOS token: ', tokenizer.decode([tokenizer.eos_token_id]))\n",
    "\n",
    "print('Model EOS token ID: ', model.config.eos_token_id)\n",
    "print('Model EOS token: ', tokenizer.decode([model.config.eos_token_id]))\n",
    "\n",
    "print('Model BOS token ID: ', model.config.eos_token_id)\n",
    "print('Model BOS token: ', tokenizer.decode([model.config.eos_token_id]))\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INST] Who are you? [/INST]\n",
      "eval_model is on: (next(eval_model.parameters()).device)\n",
      "input_ids are on: <inputs [\"input_ids\"].device)\n",
      "<s> [INST] Who are you? [/INST]\n",
      "\n",
      "[INST1] Hello, this is [NAME]. What's up?\n",
      "You look pretty, I mean really nice. Do you want to go out tonight? I want us to hang out at the bar.\n",
      "How about\n",
      "\n",
      "\n",
      "\n",
      "[INST] What is the meaning of life? [/INST]\n",
      "eval_model is on: (next(eval_model.parameters()).device)\n",
      "input_ids are on: <inputs [\"input_ids\"].device)\n",
      "<s> [INST] What is the meaning of life? [/INST]\n",
      "\n",
      "The meaning is that we must look within ourselves and make a choice about how we wish to use our life on earth. We can choose to be a good person, to contribute to society, or to live life for ourselves. This quote encourag\n",
      "\n",
      "\n",
      "\n",
      "[INST] What is the purpose of building AI? [/INST]\n",
      "eval_model is on: (next(eval_model.parameters()).device)\n",
      "input_ids are on: <inputs [\"input_ids\"].device)\n",
      "<s> [INST] What is the purpose of building AI? [/INST]\n",
      "<|user|>\n",
      "The purpose is to create a more efficient and effective means of decision-making in any industry. For instance, if a factory produces a product, the company may want to use AIs to identify weaknesses in\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import gc\n",
    "# Define a stream\n",
    "def stream(user_prompt, model_type, checkpoint=''):\n",
    "\n",
    "    if model_type == 'base':\n",
    "        eval_model = model\n",
    "\n",
    "    elif model_type == 'fine-tuned':\n",
    "        eval_model = PeftModel.from_pretrained(model, checkpoint)  # Assuming PeftModel is a pre-trained model class\n",
    "        eval_model = eval_model.to(\"cuda\")\n",
    "\n",
    "        for n, p in eval_model.named_parameters():\n",
    "            if p.device.type == \"cpu\":\n",
    "                print(f\"{n} is on cpu!\")\n",
    "\n",
    "    else:\n",
    "        print(\"You must set the model_type to base or fine-tuned'\")\n",
    "        exit()  # or raise an exception\n",
    "\n",
    "    #print(f'Proceeding to inference with peft adapters from {checkpoint}')\n",
    "\n",
    "    eval_model.config.use_cache = True\n",
    "\n",
    "    # Llama style\n",
    "    system_prompt = \"\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    prompt = f\"{B_INST} {user_prompt.strip()} {E_INST}\"\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    inputs = tokenizer((prompt), return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs [\"token_type_ids\"]\n",
    "\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    print(f'eval_model is on: (next(eval_model.parameters()).device)') # Debug i\n",
    "    print(f'input_ids are on: <inputs [\"input_ids\"].device)') # Debug line\n",
    "\n",
    "    # Despite returning the usual output, the streamer will also print the gener\n",
    "    # = eval_model.generate(**inputs, streamer-streamer)\n",
    "\n",
    "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=50,\n",
    "                          no_repeat_ngram_size=2, top_p=0.9, num_beams=1, do_sample=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def evaluation(model_type, checkpoint=''):\n",
    "  questions = [\n",
    "      \"Who are you?\",\n",
    "      \"What is the meaning of life?\",\n",
    "      \"What is the purpose of building AI?\"\n",
    "  ]\n",
    "\n",
    "  answers = [\n",
    "      \"\",\n",
    "      \"\",\n",
    "      \"\"\n",
    "  ]\n",
    "  for question, answer in zip(questions, answers):\n",
    "    stream(question, model_type, checkpoint)\n",
    "    print('\\n\\n')\n",
    "\n",
    "print(model.generation_config)\n",
    "\n",
    "evaluation(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = \"insub/imdb_prefix20_forDPO_gpt2-large-imdb-FT_siebert_sentiment-roberta-large-english\"\n",
    "dataset = \"objects76/Anthropic-hh-rlhf-dpo\"\n",
    "data = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 518, 25580, 29962, 1724, 526, 777, 274, 1558, 3838, 297, 3033, 1674, 29973, 518, 29914, 25580, 29962, 2266, 30010, 29879, 385, 28907, 1051, 29889, 13, 13, 7900, 29892, 270, 860, 29892, 6494, 914, 29892, 274, 2390, 29892, 285, 2707, 29892, 528, 277, 29892, 289, 2335, 29892, 7013, 29881, 29892, 528, 277, 2813, 29892, 528, 277, 23156, 29892, 885, 5450, 398, 29892, 274, 1657, 29892, 377, 487, 29892, 285, 29583, 29892, 528, 277, 29899, 29872, 1218, 29892, 13299, 29892, 13299, 21454, 29892, 285, 2707, 29876, 688, 657, 29892, 541, 386, 1772, 29892, 772, 459, 29892, 28015, 465, 29892, 1302, 384, 2146, 4937, 29892, 408, 845, 1772, 29892, 7339, 16846, 29876, 29892, 282, 790, 29892, 269, 17858, 29892, 13031, 29892, 281, 804, 29892, 432, 1608, 29892, 13299, 29899, 2146, 384, 292, 29892, 286, 579, 9265, 403, 29892, 285, 351, 7085, 29892, 712, 261, 29892, 432, 4981, 29892, 432, 4981, 29899, 1406, 292, 29892, 16810, 12356, 29892, 2243, 329, 29892, 923, 1008, 29892, 363, 7823, 1061, 29892, 5685, 29877, 1537, 29892, 7990, 1627, 29892, 24777, 29892, 3600, 8357, 293, 29892, 20892, 290, 568, 29892, 286, 3690, 29892, 286, 3304, 30010, 29879, 8023, 29892, 285, 351, 7085, 29892, 639, 1765, 29892, 712, 261, 29892, 885, 3774, 351, 29892, 289, 2335, 17094, 25580, 29962, 1724, 29915, 29879, 596, 25448, 697, 29973, 518, 29914, 25580, 29962]\n",
      "Decoded text: <s> [INST] What are some cuss words in english? [/INST] Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,[INST] What's your favorite one? [/INST]\n"
     ]
    }
   ],
   "source": [
    "#extract text from first row of test\n",
    "text = data['train'][0]['prompt']\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text, add_special_tokens = True)\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "# Print tokens and decoded text\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming the test split to the first 960 rows\n",
    "data['test'] = data['test'].select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./abstractor-dpo-lora/TinyLlama-1.1B-Chat-v0.6_Anthropic-hh-rlhf-dpo_0.1_epochs_512_length_ROB-DPO\n"
     ]
    }
   ],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "dataset_name = dataset.split(\"/\")[-1]\n",
    "\n",
    "context_length = 512\n",
    "grad_accum = 20\n",
    "batch_size = 1\n",
    "fine_tune_tag = 'ROB-DPO'\n",
    "\n",
    "epochs = 0.1\n",
    "drive_base_path = './abstractor-dpo-lora'\n",
    "\n",
    "# Update the save_dir path\n",
    "save_dir = f'{drive_base_path}/{model_name}_{dataset_name}_{epochs}_epochs_{context_length}_length_{fine_tune_tag}'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.29 GiB is allocated by PyTorch, and 421.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 32\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPOTrainer\n\u001b[0;32m      4\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     lr_scheduler_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#model_ref,\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#peft_config = peft_config,\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:247\u001b[0m, in \u001b[0;36mDPOTrainer.__init__\u001b[1;34m(self, model, ref_model, beta, loss_type, args, data_collator, label_pad_token_id, padding_value, truncation_mode, train_dataset, eval_dataset, tokenizer, model_init, callbacks, optimizers, preprocess_logits_for_metrics, max_length, max_prompt_length, max_target_length, peft_config, is_encoder_decoder, disable_dropout, generate_during_eval, compute_metrics, model_init_kwargs, ref_model_init_kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_reference_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_collator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\trl\\models\\modeling_base.py:582\u001b[0m, in \u001b[0;36mcreate_reference_model\u001b[1;34m(model, num_shared_layers, pattern)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed ZeRO-3 is enabled and is not compatible with `create_reference_model()`. Please instantiate your reference model directly with `AutoCausalLM.from_pretrained()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    581\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()]\n\u001b[1;32m--> 582\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# if no layers are shared, return copy of model\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_shared_layers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\parameter.py:58\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m     59\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.29 GiB is allocated by PyTorch, and 421.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"Results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=0.25,\n",
    "    #optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    log_level=\"debug\",\n",
    "    save_steps=0.25,\n",
    "\n",
    "    logging_steps=10,\n",
    "    # learning_rate=5e-7,\n",
    "\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=epochs,\n",
    "\n",
    "    # max_steps=steps,\n",
    "    # warmup_steps=20,\n",
    "\n",
    "    # ir_scheduler_type=\"linear\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    #model_ref,\n",
    "    args = training_arguments,\n",
    "    beta = 0.1,\n",
    "    #peft_config = peft_config,\n",
    "    train_dataset = data['train'],\n",
    "    eval_dataset = data['test'],\n",
    "    tokenizer = tokenizer,\n",
    "\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
