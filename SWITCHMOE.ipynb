{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH TRANSFORMER / MoE :\n",
    "\n",
    "1. **Expert Level:** Each expert consists of three sub-models - a Transformer with DPO, RAG, and MAMBA. This level focuses on text-based input and is designed to handle specific domains or types of queries with high proficiency.\n",
    "\n",
    "\n",
    "\n",
    "2. **Switch Transformer MoE (Mixture of Experts) Level:** This layer integrates multiple Experts, each fine-tuned for different domains or tasks. The Switch Transformer directs queries to the most relevant Expert based on the context, leveraging the specialized skills of each component model.\n",
    "\n",
    "\n",
    "\n",
    "3. **King Model Level:** Each King Model represents a multimodal system, incorporating separate MoE structures for different types of data, such as text, images, videos, and audio. This approach allows the King Model to process and understand a wide range of inputs, potentially enabling richer and more nuanced responses.\n",
    "\n",
    "\n",
    "\n",
    "4. **God Model Level:** At this level, multiple King Models, each trained in different domains (e.g., science, literature, art), are integrated. The God Model can draw on a vast pool of domain-specific knowledge and multimodal understanding, making it highly versatile and capable of handling complex, multi-faceted queries.\n",
    "\n",
    "\n",
    "\n",
    "5. **Multiverse Model Level:** This ultimate layer aggregates multiple God Models, each representing different spheres of knowledge or different approaches to intelligence. Such a system could theoretically possess an extraordinarily broad and deep understanding of the world, resembling AGI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE TRANSFORMER AND DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "# Define vocabulary size and dummy data parameters\n",
    "NUM_WORDS = 1000  # Example vocabulary size\n",
    "sequence_length = 30  # Sequence length for the LanguageDataset\n",
    "dummy_data_size = 1000  # Total number of tokens in the dummy dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model instance\n",
    "model = LanguageModelTransformer(\n",
    "    vocab_size=vocab_size,  # Use the vocab size from the tokenizer\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    max_length=100,\n",
    "    rank=16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate a new alpha value based on the current loss.\n",
    "    \"\"\"\n",
    "    if current_loss >= initial_loss:\n",
    "        return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "    loss_ratio = current_loss / initial_loss\n",
    "    alpha_range = initial_alpha - final_alpha\n",
    "    new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "    return new_alpha\n",
    "\n",
    "# Enable QLORA during training\n",
    "model.decoder.toggle_qlora(True)\n",
    "\n",
    "initial_loss = None\n",
    "# Training loop\n",
    "# Assuming model is an instance of LanguageModelTransformer\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    model.decoder.toggle_qlora(True)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Encountered NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Set the initial_loss after the first batch of the first epoch\n",
    "        if initial_loss is None and batch_idx == 0:\n",
    "            initial_loss = loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for NaN in total_loss\n",
    "    if math.isnan(total_loss):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Update alpha at the end of each epoch based on the average loss\n",
    "    new_alpha = calculate_new_alpha(average_loss, initial_loss)\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, QLORALayer):\n",
    "            layer.update_alpha(new_alpha)\n",
    "\n",
    "    #model.decoder.toggle_qlora(False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def format_stackexchange_dpo(samples):\n",
    "    return {\n",
    "        \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "        \"chosen\": samples[\"response_j\"],   # Rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # Rated worse than j\n",
    "    }\n",
    "\n",
    "# Load and format a subset (30%) of the StackExchange DPO dataset\n",
    "dataset = load_dataset(\"lvwerra/stack-exchange-paired\")\n",
    "subset_size = int(0.3 * len(dataset['train']))  # 30% of the dataset\n",
    "subset_indices = torch.randperm(len(dataset['train'])).tolist()[:subset_size]  # Randomly select indices\n",
    "formatted_dataset = dataset['train'].select(subset_indices).map(format_stackexchange_dpo, batched=True, load_from_cache_file=False)\n",
    "\n",
    "# Convert formatted dataset to DataLoader for batch processing\n",
    "dpo_dataloader = DataLoader(formatted_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MarginRankingLoss\n",
    "\n",
    "# Define DPO-specific loss function\n",
    "dpo_loss_function = MarginRankingLoss(margin=1.0)\n",
    "dpo_num_epochs = 2  # Define the number of epochs for DPO training\n",
    "\n",
    "# DPO Training loop\n",
    "for epoch in range(dpo_num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_dpo_loss = 0\n",
    "\n",
    "    for batch in dpo_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare the input for the model\n",
    "        prompts = batch['prompt'].to(device)\n",
    "        preferred_responses = batch['chosen'].to(device)\n",
    "        less_preferred_responses = batch['rejected'].to(device)\n",
    "\n",
    "        # Forward pass and model's scoring mechanism for responses\n",
    "        # The model should output scores for the preferred and less-preferred responses\n",
    "        output_preferred = model(preferred_responses)\n",
    "        output_less_preferred = model(less_preferred_responses)\n",
    "\n",
    "        # Compute DPO loss\n",
    "        dpo_loss = dpo_loss_function(output_preferred, output_less_preferred, torch.ones(output_preferred.size(0)).to(device))\n",
    "        total_dpo_loss += dpo_loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        dpo_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{dpo_num_epochs}, DPO Loss: {total_dpo_loss / len(dpo_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.value: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.keys: Linear(in_features=32, out_features=32, bias=False)\n",
      "self.queries: Linear(in_features=32, out_features=32, bias=False)\n",
      "values_reshaped: torch.Size([8, 512, 8, 32])\n",
      "keys_reshaped: torch.Size([8, 512, 8, 32])\n",
      "queries_reshaped: torch.Size([8, 512, 8, 32])\n",
      "values: torch.Size([8, 512, 8, 32])\n",
      "keys: torch.Size([8, 512, 8, 32])\n",
      "queries: torch.Size([8, 512, 8, 32])\n",
      "attention_scores: torch.Size([8, 8, 512, 512])\n",
      "keys_window: torch.Size([8, 26, 8, 32])\n",
      "queries_window: torch.Size([8, 26, 8, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript h has size 8 for operand 1 which does not broadcast with previously seen size 26",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 318\u001b[0m\n\u001b[0;32m    315\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# Shifted input IDs for labels\u001b[39;00m\n\u001b[0;32m    320\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_ids[:, \u001b[38;5;241m1\u001b[39m:], input_ids[:, :\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 266\u001b[0m, in \u001b[0;36mMAMBA.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    262\u001b[0m lstm_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m--> 266\u001b[0m     x, states, lstm_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 184\u001b[0m, in \u001b[0;36mModifiedTransformerBlock.forward\u001b[1;34m(self, value, key, query, mask, states, lstm_hidden)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, key, query, mask, states, lstm_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 184\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention \u001b[38;5;241m+\u001b[39m query))\n\u001b[0;32m    188\u001b[0m     processed_states, new_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_layer(x, states)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 78\u001b[0m, in \u001b[0;36mSparseAttention.forward\u001b[1;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqueries\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Calculate attention scores\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_sparse_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[32], line 116\u001b[0m, in \u001b[0;36mSparseAttention.calculate_sparse_attention\u001b[1;34m(self, queries, keys, window_size)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries_window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqueries_window\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Compute scores using einsum\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnhqd,nwhd->nhqw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mqueries_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_window\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Place the calculated scores in their respective positions\u001b[39;00m\n\u001b[0;32m    119\u001b[0m attention_scores[:, :, i, start:end] \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\functional.py:372\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    369\u001b[0m     _operands \u001b[38;5;241m=\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_operands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript h has size 8 for operand 1 which does not broadcast with previously seen size 26"
     ]
    }
   ],
   "source": [
    "#MAMBA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "# Tokenizer and Dataset - same as your working code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# Define MAMBA Model with appropriate hyperparameters\n",
    "#vocab_size = len(tokenizer)  # Get the correct vocab size from the tokenizer\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, heads, window_size):\n",
    "\n",
    "        super(SparseAttention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        print(f\"self.value: {self.values}\")\n",
    "\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        print(f\"self.keys: {self.keys}\")\n",
    "\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        print(f\"self.queries: {self.queries}\")\n",
    "\n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, value_len, key_len, query_len = values.size(0), values.size(1), keys.size(1), query.size(1)\n",
    "        \n",
    "        # Reshape values, keys, and queries for multi-head attention\n",
    "        values_reshaped = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        print(f\"values_reshaped: {values_reshaped.shape}\")\n",
    "        keys_reshaped = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        print(f\"keys_reshaped: {keys_reshaped.shape}\")\n",
    "        queries_reshaped = query.view(N, query_len, self.heads, self.head_dim)\n",
    "        print(f\"queries_reshaped: {queries_reshaped.shape}\")\n",
    "\n",
    "        # Apply linear layers\n",
    "        values = self.values(values_reshaped)\n",
    "        print(f\"values: {values.shape}\")\n",
    "        keys = self.keys(keys_reshaped)\n",
    "        print(f\"keys: {keys.shape}\")\n",
    "        queries = self.queries(queries_reshaped)\n",
    "        print(f\"queries: {queries.shape}\")\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention = self.calculate_sparse_attention(queries, keys, self.window_size)\n",
    "        print(f\"attention: {attention.shape}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax and calculate the output\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).view(N, query_len, self.heads * self.head_dim)\n",
    "\n",
    "        # Before the final linear layer\n",
    "        print(\"Output shape before fc_out:\", out.shape)\n",
    "        out = self.fc_out(out)\n",
    "        print(\"Output shape after fc_out:\", out.shape)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def calculate_sparse_attention(self, queries, keys, window_size):\n",
    "        N, query_len, heads, head_dim = queries.size()\n",
    "        _, seq_len, _, _ = keys.size()\n",
    "\n",
    "        # Initialize attention scores with negative infinity for sparsity\n",
    "        attention_scores = torch.full((N, heads, query_len, seq_len), float('-inf'), device=queries.device)\n",
    "        print(f\"attention_scores: {attention_scores.shape}\")\n",
    "        # Iterate over each position in the sequence\n",
    "        for i in range(seq_len):\n",
    "            # Determine the local window range\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(seq_len, i + window_size + 1)\n",
    "\n",
    "            # Calculate attention scores within the window\n",
    "            keys_window = keys[:, start:end, :, :]\n",
    "            print(f\"keys_window: {keys_window.shape}\")\n",
    "            # Adjust queries_window slicing to be aligned with the keys_window\n",
    "            queries_window = queries[:, i:i+1, :, :].expand(-1, end - start, -1, -1)\n",
    "            print(f\"queries_window: {queries_window.shape}\")\n",
    "\n",
    "            # Compute scores using einsum\n",
    "            scores = torch.einsum(\"nhqd,nwhd->nhqw\", [queries_window, keys_window])\n",
    "\n",
    "            # Place the calculated scores in their respective positions\n",
    "            attention_scores[:, :, i, start:end] = scores.squeeze(2)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SSMLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, state_dim):\n",
    "\n",
    "        super(SSMLayer, self).__init__()\n",
    "\n",
    "        self.state_update = nn.Linear(input_dim, state_dim)\n",
    "\n",
    "        self.state_process = nn.Linear(state_dim, input_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, states):\n",
    "\n",
    "        new_states = self.state_update(x) + states\n",
    "\n",
    "        processed_states = self.state_process(new_states)\n",
    "\n",
    "        return processed_states, new_states\n",
    "\n",
    "\n",
    "class ModifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, state_dim, dropout, forward_expansion, lstm_dim, window_size):\n",
    "        \n",
    "        super(ModifiedTransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = SparseAttention(embed_size, heads, window_size)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.ssm_layer = SSMLayer(embed_size, state_dim)\n",
    "\n",
    "\n",
    "\n",
    "        # LSTM Layer\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_dim, batch_first=True)\n",
    "\n",
    "        \n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "\n",
    "            nn.Linear(embed_size + lstm_dim, forward_expansion * embed_size),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, value, key, query, mask, states, lstm_hidden=None):\n",
    "\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "        processed_states, new_states = self.ssm_layer(x, states)\n",
    "\n",
    "        \n",
    "\n",
    "        # Pass the output through LSTM layer\n",
    "\n",
    "        lstm_output, new_lstm_hidden = self.lstm(x, lstm_hidden)\n",
    "\n",
    "        x = torch.cat((x, lstm_output), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out, new_states, new_lstm_hidden\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_blocks, heads, state_dim, lstm_dim, forward_expansion, max_length, dropout, window_size):\n",
    "\n",
    "        super(MAMBA, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "\n",
    "            ModifiedTransformerBlock(\n",
    "\n",
    "                embed_size=embed_size,\n",
    "\n",
    "                heads=heads,\n",
    "\n",
    "                state_dim=state_dim,\n",
    "\n",
    "                dropout=dropout,\n",
    "\n",
    "                forward_expansion=forward_expansion,\n",
    "\n",
    "                lstm_dim=lstm_dim,\n",
    "\n",
    "                window_size=window_size\n",
    "\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size + lstm_dim, vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "\n",
    "\n",
    "        states = None\n",
    "\n",
    "        lstm_hidden = None\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "\n",
    "            x, states, lstm_hidden = block(x, x, x, mask, states, lstm_hidden)\n",
    "\n",
    "\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "embed_size = 256  # Embedding size\n",
    "num_blocks = 6  # Number of transformer blocks\n",
    "heads = 8  # Number of attention heads\n",
    "state_dim = 64  # Dimension of the state in SSMLayer\n",
    "lstm_dim = 64  # Dimension of the hidden state in LSTM\n",
    "forward_expansion = 4  # Expansion factor in feed forward network\n",
    "max_length = 512  # Maximum length of the sequence\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "\n",
    "# Create MAMBA model instance\n",
    "mamba_model = MAMBA(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_blocks=num_blocks,\n",
    "    heads=heads,\n",
    "    state_dim=state_dim,\n",
    "    lstm_dim=lstm_dim,\n",
    "    forward_expansion=forward_expansion,\n",
    "    max_length=max_length,\n",
    "    dropout=dropout,\n",
    "    window_size=25  \n",
    ")\n",
    "\n",
    "# Define Device for Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = AdamW(mamba_model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    mamba_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = mamba_model(input_ids, attention_mask)\n",
    "        # Shifted input IDs for labels\n",
    "        labels = torch.cat([input_ids[:, 1:], input_ids[:, :1]], dim=1)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the tokenizer for your custom model\n",
    "\n",
    "custom_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # or another tokenizer if not BERT\n",
    "\n",
    "\n",
    "\n",
    "# Initialize your custom model (already fine-tuned with DPO)\n",
    "\n",
    "custom_model = LanguageModelTransformer(...)\n",
    "\n",
    "custom_model.load_state_dict(torch.load('path_to_fine_tuned_model.pt'))  # Load your fine-tuned model\n",
    "\n",
    "\n",
    "\n",
    "# Set up RAG components\n",
    "\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-base\")\n",
    "\n",
    "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-base\", index_name=\"custom\", passages_path=\"path_to_your_passages_file\")\n",
    "\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-base\", retriever=rag_retriever)\n",
    "\n",
    "\n",
    "\n",
    "def unified_inference_pipeline(input_text):\n",
    "\n",
    "    # Generate initial response using your custom model\n",
    "\n",
    "    inputs = custom_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = custom_model(**inputs)\n",
    "\n",
    "    response = custom_tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Augment response using RAG\n",
    "\n",
    "    rag_inputs = rag_tokenizer(response, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = rag_model.generate(rag_inputs[\"input_ids\"])\n",
    "\n",
    "    augmented_response = rag_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    return augmented_response\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "input_text = \"Your input query here\"\n",
    "\n",
    "augmented_response = unified_inference_pipeline(input_text)\n",
    "\n",
    "print(augmented_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH ROUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch Router:\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class RoutingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, response_dim, num_models):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(response_dim * num_models, num_models)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, responses):\n",
    "\n",
    "        # Concatenate responses and pass through the fully connected layer\n",
    "\n",
    "        combined_responses = torch.cat(responses, dim=1)\n",
    "\n",
    "        weights = self.fc(combined_responses)\n",
    "\n",
    "        return F.softmax(weights, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class IntegratedModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model1, model2, model3, response_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model1 = model1  # Transformer with DPO\n",
    "\n",
    "        self.model2 = model2  # RAG\n",
    "\n",
    "        self.model3 = model3  # MAMBA\n",
    "\n",
    "        self.routing_layer = RoutingLayer(response_dim, 3)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, input_prompt):\n",
    "\n",
    "        # Assuming each model returns a response vector of the same dimension\n",
    "\n",
    "        response1 = self.model1(input_prompt)\n",
    "\n",
    "        response2 = self.model2(input_prompt)\n",
    "\n",
    "        response3 = self.model3(input_prompt)\n",
    "\n",
    "\n",
    "\n",
    "        # Routing\n",
    "\n",
    "        weights = self.routing_layer([response1, response2, response3])\n",
    "\n",
    "\n",
    "\n",
    "        # Weighted sum of responses\n",
    "\n",
    "        final_response = weights[:, 0, None] * response1 + weights[:, 1, None] * response2 + weights[:, 2, None] * response3\n",
    "\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# Assuming IntegratedModel is already defined and instantiated as integrated_model\n",
    "\n",
    "# Assuming a DataLoader named 'data_loader' providing input prompts and corresponding target tokens\n",
    "\n",
    "\n",
    "\n",
    "# Define AdamW optimizer for the integrated model\n",
    "\n",
    "optimizer = optim.AdamW(integrated_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function - Cross-Entropy Loss for language modeling\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # tokenizer.pad_token_id for the padding token ID\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 5  # Number of training epochs\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    integrated_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        input_prompts, target_tokens = batch['input_prompts'], batch['target_tokens']\n",
    "\n",
    "        \n",
    "\n",
    "        # Forward pass through integrated model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = integrated_model(input_prompts)\n",
    "\n",
    "        \n",
    "\n",
    "        # Reshape for calculating loss (Cross-Entropy expects 2D input)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "\n",
    "        loss = criterion(outputs, target_tokens)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        # Backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save model state if needed\n",
    "\n",
    "torch.save(integrated_model.state_dict(), 'path_to_save_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MoESwitchTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_experts, response_dim, expert_params):\n",
    "\n",
    "        super(MoESwitchTransformer, self).__init__()\n",
    "\n",
    "        self.experts = nn.ModuleList([IntegratedModel(**expert_params) for _ in range(num_experts)])\n",
    "\n",
    "        self.routing_layer = RoutingLayer(response_dim, num_experts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_prompt):\n",
    "\n",
    "        expert_responses = [expert(input_prompt) for expert in self.experts]\n",
    "\n",
    "        # Combine responses for routing\n",
    "\n",
    "        combined_responses = torch.stack(expert_responses, dim=1)\n",
    "\n",
    "        # Routing: Shape of weights will be [batch_size, num_experts]\n",
    "\n",
    "        weights = self.routing_layer(combined_responses)\n",
    "\n",
    "        # Weighted sum of expert responses\n",
    "\n",
    "        final_response = torch.sum(weights.unsqueeze(2) * combined_responses, dim=1)\n",
    "\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n",
    "# Define the MoE model\n",
    "\n",
    "num_experts = 5  # Number of experts\n",
    "\n",
    "response_dim = 512  # Dimension of response (assumed for illustration)\n",
    "\n",
    "expert_params = {'model1': model_dpo, 'model2': model_rag, 'model3': model_mamba, 'response_dim': response_dim}\n",
    "\n",
    "\n",
    "\n",
    "moe_switch_transformer = MoESwitchTransformer(num_experts, response_dim, expert_params)\n",
    "\n",
    "\n",
    "\n",
    "# Define an input prompt (for example purposes)\n",
    "\n",
    "input_prompt = torch.rand(1, 512)  # Replace with actual input\n",
    "\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "output = moe_switch_transformer(input_prompt)\n",
    "\n",
    "print(output.shape)  # Output shape will depend on the response_dim\n",
    "\n",
    "\n",
    "\n",
    "# Training the MoE Switch Transformer\n",
    "\n",
    "# Define optimizer and loss function as before\n",
    "\n",
    "optimizer = torch.optim.AdamW(moe_switch_transformer.parameters(), lr=5e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Define according to your specific task\n",
    "\n",
    "\n",
    "\n",
    "# Assume 'data_loader' is provided\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    moe_switch_transformer.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, targets = batch  # Assuming batch contains inputs and targets\n",
    "\n",
    "        outputs = moe_switch_transformer(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
