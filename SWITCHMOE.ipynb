{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH TRANSFORMER / MoE :\n",
    "\n",
    "1. **Expert Level:** Each expert consists of three sub-models - a Transformer with DPO, RAG, and MAMBA. This level focuses on text-based input and is designed to handle specific domains or types of queries with high proficiency.\n",
    "\n",
    "\n",
    "\n",
    "2. **Switch Transformer MoE (Mixture of Experts) Level:** This layer integrates multiple Experts, each fine-tuned for different domains or tasks. The Switch Transformer directs queries to the most relevant Expert based on the context, leveraging the specialized skills of each component model.\n",
    "\n",
    "\n",
    "\n",
    "3. **King Model Level:** Each King Model represents a multimodal system, incorporating separate MoE structures for different types of data, such as text, images, videos, and audio. This approach allows the King Model to process and understand a wide range of inputs, potentially enabling richer and more nuanced responses.\n",
    "\n",
    "\n",
    "\n",
    "4. **God Model Level:** At this level, multiple King Models, each trained in different domains (e.g., science, literature, art), are integrated. The God Model can draw on a vast pool of domain-specific knowledge and multimodal understanding, making it highly versatile and capable of handling complex, multi-faceted queries.\n",
    "\n",
    "\n",
    "\n",
    "5. **Multiverse Model Level:** This ultimate layer aggregates multiple God Models, each representing different spheres of knowledge or different approaches to intelligence. Such a system could theoretically possess an extraordinarily broad and deep understanding of the world, resembling AGI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE TRANSFORMER AND DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "# Define vocabulary size and dummy data parameters\n",
    "NUM_WORDS = 1000  # Example vocabulary size\n",
    "sequence_length = 30  # Sequence length for the LanguageDataset\n",
    "dummy_data_size = 1000  # Total number of tokens in the dummy dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model instance\n",
    "model = LanguageModelTransformer(\n",
    "    vocab_size=vocab_size,  # Use the vocab size from the tokenizer\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    max_length=100,\n",
    "    rank=16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate a new alpha value based on the current loss.\n",
    "    \"\"\"\n",
    "    if current_loss >= initial_loss:\n",
    "        return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "    loss_ratio = current_loss / initial_loss\n",
    "    alpha_range = initial_alpha - final_alpha\n",
    "    new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "    return new_alpha\n",
    "\n",
    "# Enable QLORA during training\n",
    "model.decoder.toggle_qlora(True)\n",
    "\n",
    "initial_loss = None\n",
    "# Training loop\n",
    "# Assuming model is an instance of LanguageModelTransformer\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    model.decoder.toggle_qlora(True)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Encountered NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Set the initial_loss after the first batch of the first epoch\n",
    "        if initial_loss is None and batch_idx == 0:\n",
    "            initial_loss = loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for NaN in total_loss\n",
    "    if math.isnan(total_loss):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Update alpha at the end of each epoch based on the average loss\n",
    "    new_alpha = calculate_new_alpha(average_loss, initial_loss)\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, QLORALayer):\n",
    "            layer.update_alpha(new_alpha)\n",
    "\n",
    "    #model.decoder.toggle_qlora(False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def format_stackexchange_dpo(samples):\n",
    "    return {\n",
    "        \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "        \"chosen\": samples[\"response_j\"],   # Rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # Rated worse than j\n",
    "    }\n",
    "\n",
    "# Load and format a subset (30%) of the StackExchange DPO dataset\n",
    "dataset = load_dataset(\"lvwerra/stack-exchange-paired\")\n",
    "subset_size = int(0.3 * len(dataset['train']))  # 30% of the dataset\n",
    "subset_indices = torch.randperm(len(dataset['train'])).tolist()[:subset_size]  # Randomly select indices\n",
    "formatted_dataset = dataset['train'].select(subset_indices).map(format_stackexchange_dpo, batched=True, load_from_cache_file=False)\n",
    "\n",
    "# Convert formatted dataset to DataLoader for batch processing\n",
    "dpo_dataloader = DataLoader(formatted_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MarginRankingLoss\n",
    "\n",
    "# Define DPO-specific loss function\n",
    "dpo_loss_function = MarginRankingLoss(margin=1.0)\n",
    "dpo_num_epochs = 2  # Define the number of epochs for DPO training\n",
    "\n",
    "# DPO Training loop\n",
    "for epoch in range(dpo_num_epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    total_dpo_loss = 0\n",
    "\n",
    "    for batch in dpo_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare the input for the model\n",
    "        prompts = batch['prompt'].to(device)\n",
    "        preferred_responses = batch['chosen'].to(device)\n",
    "        less_preferred_responses = batch['rejected'].to(device)\n",
    "\n",
    "        # Forward pass and model's scoring mechanism for responses\n",
    "        # The model should output scores for the preferred and less-preferred responses\n",
    "        output_preferred = model(preferred_responses)\n",
    "        output_less_preferred = model(less_preferred_responses)\n",
    "\n",
    "        # Compute DPO loss\n",
    "        dpo_loss = dpo_loss_function(output_preferred, output_less_preferred, torch.ones(output_preferred.size(0)).to(device))\n",
    "        total_dpo_loss += dpo_loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        dpo_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{dpo_num_epochs}, DPO Loss: {total_dpo_loss / len(dpo_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 30522\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "Shape of x : torch.Size([8, 512, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 320]' is invalid for input of size 1048576",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 248\u001b[0m\n\u001b[0;32m    245\u001b[0m mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, heads, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Flatten outputs and input ids\u001b[39;00m\n\u001b[0;32m    251\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 196\u001b[0m, in \u001b[0;36mMAMBA.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    193\u001b[0m     x, states, lstm_hidden \u001b[38;5;241m=\u001b[39m block(x, x, x, mask, states, lstm_hidden) \u001b[38;5;66;03m# Update x with the output from each block\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of x : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlstm_dim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure correct reshaping\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#print(f\"Shape after reshaping for fc_out: {x.shape}\")\u001b[39;00m\n\u001b[0;32m    199\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 320]' is invalid for input of size 1048576"
     ]
    }
   ],
   "source": [
    "#MAMBA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def get_valid_indices(sequence_length, window_size):\n",
    "    all_indices = list(range(window_size * 2 + 1))\n",
    "    valid_indices = []\n",
    "    for idx in all_indices:\n",
    "        if idx < sequence_length and idx >= 0:\n",
    "            valid_indices.append(idx)\n",
    "    return torch.tensor(valid_indices, dtype=torch.long)\n",
    "\n",
    "# Tokenizer and Dataset - same as your working code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "# Get the vocab size of the tokenizer\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, window_size):\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, _, _ = values.size()\n",
    "\n",
    "        # Reshape values, keys, and queries for multi-head attention\n",
    "        values = self.values(values.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "        keys = self.keys(keys.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "        queries = self.queries(query.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention = self.calculate_sparse_attention(queries, keys, self.window_size)\n",
    "\n",
    "\n",
    "        # Update mask for attention scores\n",
    "        if mask is not None:\n",
    "            # Reshape mask to be broadcastable to [N, heads, query_len, seq_len]\n",
    "            mask = mask.unsqueeze(2)  # Adding dimension for query_len\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax and calculate the output\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, -1, self.heads * self.head_dim)\n",
    "\n",
    "        # Before the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def calculate_sparse_attention(self, queries, keys, window_size):\n",
    "        N, query_len, heads, head_dim = queries.size()\n",
    "        _, seq_len, _, _ = keys.size()\n",
    "        attention_scores = torch.zeros((N, heads, query_len, seq_len), device=queries.device)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(seq_len, i + window_size + 1)\n",
    "\n",
    "            keys_window = keys[:, start:end, :, :]\n",
    "            queries_window = queries[:, i:i+1, :, :].repeat(1, end - start, 1, 1)\n",
    "\n",
    "            for h in range(heads):\n",
    "                keys_head = keys_window[:, :, h, :]\n",
    "                queries_head = queries_window[:, :, h, :]\n",
    "\n",
    "                keys_head_reshaped = keys_head.reshape(N, end - start, head_dim)\n",
    "                queries_head_reshaped = queries_head.reshape(N, end - start, head_dim)\n",
    "\n",
    "                scores_head = torch.bmm(queries_head_reshaped, keys_head_reshaped.transpose(1, 2))\n",
    "\n",
    "                valid_indices = get_valid_indices(end - start, window_size)\n",
    "                broadcast_mask = torch.zeros_like(scores_head, device=queries.device)\n",
    "                broadcast_mask[..., valid_indices] = 1\n",
    "                summed_scores_head = scores_head.sum(dim=2)\n",
    "                summed_mask = broadcast_mask.sum(dim=2)\n",
    "                attention_scores[:, h, i, start:end] = summed_scores_head / summed_mask.clamp(min=1.)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "class SSMLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SSMLayer, self).__init__()\n",
    "        self.state_update = nn.Linear(input_dim, input_dim)\n",
    "        self.state_process = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if states is None:\n",
    "            states = torch.zeros_like(x)\n",
    "\n",
    "        new_states = self.state_update(x) + states\n",
    "        processed_states = self.state_process(new_states)\n",
    "\n",
    "        return processed_states, new_states\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, lstm_dim, window_size):\n",
    "        super(ModifiedTransformerBlock, self).__init__()\n",
    "        self.attention = SparseAttention(embed_size, heads, window_size)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ssm_layer = SSMLayer(embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_dim, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size + lstm_dim, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Output dimension is embed_size\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, value, key, query, mask, states, lstm_hidden=None):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        processed_states, new_states = self.ssm_layer(x, states)\n",
    "\n",
    "        if lstm_hidden is None:\n",
    "            lstm_hidden = (torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device),\n",
    "                        torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device))\n",
    "\n",
    "        lstm_output, new_lstm_hidden = self.lstm(x, lstm_hidden)\n",
    "        print(f\"LSTM output shape: {lstm_output.shape}\")\n",
    "        x_concat = torch.cat((x, lstm_output), dim=-1)\n",
    "        print(f\"x_concat: {x_concat.shape}\")\n",
    "        forward = self.feed_forward(x_concat)\n",
    "        out = self.dropout(self.norm2(forward))\n",
    "\n",
    "        return out, new_states, new_lstm_hidden\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_blocks, heads, lstm_dim, forward_expansion, max_length, dropout, window_size):\n",
    "        super(MAMBA, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            ModifiedTransformerBlock(\n",
    "                embed_size=embed_size,\n",
    "                heads=heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "                lstm_dim=lstm_dim,\n",
    "                window_size=window_size\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adjust the output dimension of the final linear layer to match the concatenated output\n",
    "        self.fc_out = nn.Linear(embed_size + lstm_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        lstm_hidden = None\n",
    "        states = None  # Initialize states as None\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x, states, lstm_hidden = block(x, x, x, mask, states, lstm_hidden) # Update x with the output from each block\n",
    "\n",
    "        print(f\"Shape of x : {x.shape}\")\n",
    "        x = x.view(-1, embed_size + lstm_dim)  # Ensure correct reshaping\n",
    "        #print(f\"Shape after reshaping for fc_out: {x.shape}\")\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        print(f\"Shape of output of Mamba: {out.shape}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "embed_size = 256  # Embedding size\n",
    "num_blocks = 6  # Number of transformer blocks\n",
    "heads = 8  # Number of attention heads\n",
    "state_dim = 64  # Dimension of the state in SSMLayer\n",
    "lstm_dim = 64  # Dimension of the hidden state in LSTM\n",
    "forward_expansion = 4  # Expansion factor in feed forward network\n",
    "max_length = 512  # Maximum length of the sequence\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "\n",
    "# Create MAMBA model instance\n",
    "mamba_model = MAMBA(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_blocks=num_blocks,\n",
    "    heads=heads,\n",
    "    lstm_dim=lstm_dim,\n",
    "    forward_expansion=forward_expansion,\n",
    "    max_length=max_length,\n",
    "    dropout=dropout,\n",
    "    window_size=25  \n",
    ")\n",
    "\n",
    "# Define Device for Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = AdamW(mamba_model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    mamba_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        # Create dummy mask when no attention_mask is given\n",
    "        mask = attention_mask.unsqueeze(1).repeat(1, heads, 1) if attention_mask is not None else None\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mamba_model(input_ids, mask)\n",
    "\n",
    "        # Flatten outputs and input ids\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        input_ids = input_ids.view(-1)\n",
    "\n",
    "        # Shift input_ids along the sequence axis to create labels\n",
    "        labels = torch.roll(input_ids, -1, dims=0)\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 30522\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "LSTM output shape: torch.Size([8, 512, 64])\n",
      "x_concat: torch.Size([8, 512, 320])\n",
      "Shape of x before fc_out: torch.Size([8, 512, 256])\n",
      "Shape after reshaping for fc_out: torch.Size([64, 256, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16384x64 and 320x30522)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 246\u001b[0m\n\u001b[0;32m    243\u001b[0m mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, heads, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Flatten outputs and input ids\u001b[39;00m\n\u001b[0;32m    249\u001b[0m outputs_flattened \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 198\u001b[0m, in \u001b[0;36mMAMBA.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    196\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, embed_size, lstm_dim) \u001b[38;5;66;03m# Ensure correct reshaping\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape after reshaping for fc_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 198\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16384x64 and 320x30522)"
     ]
    }
   ],
   "source": [
    "#MAMBA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def get_valid_indices(sequence_length, window_size):\n",
    "    all_indices = list(range(window_size * 2 + 1))\n",
    "    valid_indices = []\n",
    "    for idx in all_indices:\n",
    "        if idx < sequence_length and idx >= 0:\n",
    "            valid_indices.append(idx)\n",
    "    return torch.tensor(valid_indices, dtype=torch.long)\n",
    "\n",
    "# Tokenizer and Dataset - same as your working code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "# Get the vocab size of the tokenizer\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, window_size):\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, _, _ = values.size()\n",
    "\n",
    "        # Reshape values, keys, and queries for multi-head attention\n",
    "        values = self.values(values.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "        keys = self.keys(keys.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "        queries = self.queries(query.view(N, -1, self.head_dim)).view(N, -1, self.heads, self.head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention = self.calculate_sparse_attention(queries, keys, self.window_size)\n",
    "\n",
    "\n",
    "        # Update mask for attention scores\n",
    "        if mask is not None:\n",
    "            # Reshape mask to be broadcastable to [N, heads, query_len, seq_len]\n",
    "            mask = mask.unsqueeze(2)  # Adding dimension for query_len\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax and calculate the output\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, -1, self.heads * self.head_dim)\n",
    "\n",
    "        # Before the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def calculate_sparse_attention(self, queries, keys, window_size):\n",
    "        N, query_len, heads, head_dim = queries.size()\n",
    "        _, seq_len, _, _ = keys.size()\n",
    "        attention_scores = torch.zeros((N, heads, query_len, seq_len), device=queries.device)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(seq_len, i + window_size + 1)\n",
    "\n",
    "            keys_window = keys[:, start:end, :, :]\n",
    "            queries_window = queries[:, i:i+1, :, :].repeat(1, end - start, 1, 1)\n",
    "\n",
    "            for h in range(heads):\n",
    "                keys_head = keys_window[:, :, h, :]\n",
    "                queries_head = queries_window[:, :, h, :]\n",
    "\n",
    "                keys_head_reshaped = keys_head.reshape(N, end - start, head_dim)\n",
    "                queries_head_reshaped = queries_head.reshape(N, end - start, head_dim)\n",
    "\n",
    "                scores_head = torch.bmm(queries_head_reshaped, keys_head_reshaped.transpose(1, 2))\n",
    "\n",
    "                valid_indices = get_valid_indices(end - start, window_size)\n",
    "                broadcast_mask = torch.zeros_like(scores_head, device=queries.device)\n",
    "                broadcast_mask[..., valid_indices] = 1\n",
    "                summed_scores_head = scores_head.sum(dim=2)\n",
    "                summed_mask = broadcast_mask.sum(dim=2)\n",
    "                attention_scores[:, h, i, start:end] = summed_scores_head / summed_mask.clamp(min=1.)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "class SSMLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SSMLayer, self).__init__()\n",
    "        self.state_update = nn.Linear(input_dim, input_dim)\n",
    "        self.state_process = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if states is None:\n",
    "            states = torch.zeros_like(x)\n",
    "\n",
    "        new_states = self.state_update(x) + states\n",
    "        processed_states = self.state_process(new_states)\n",
    "\n",
    "        return processed_states, new_states\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, lstm_dim, window_size):\n",
    "        super(ModifiedTransformerBlock, self).__init__()\n",
    "        self.attention = SparseAttention(embed_size, heads, window_size)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ssm_layer = SSMLayer(embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_dim, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size + lstm_dim, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Output dimension is embed_size\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, value, key, query, mask, states, lstm_hidden=None):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        processed_states, new_states = self.ssm_layer(x, states)\n",
    "\n",
    "        if lstm_hidden is None:\n",
    "            lstm_hidden = (torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device),\n",
    "                        torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device))\n",
    "\n",
    "        lstm_output, new_lstm_hidden = self.lstm(x, lstm_hidden)\n",
    "        print(f\"LSTM output shape: {lstm_output.shape}\")\n",
    "        x_concat = torch.cat((x, lstm_output), dim=-1)\n",
    "        print(f\"x_concat: {x_concat.shape}\")\n",
    "        forward = self.feed_forward(x_concat)\n",
    "        out = self.dropout(self.norm2(forward))\n",
    "\n",
    "        return out, new_states, new_lstm_hidden\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_blocks, heads, lstm_dim, forward_expansion, max_length, dropout, window_size):\n",
    "        super(MAMBA, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            ModifiedTransformerBlock(\n",
    "                embed_size=embed_size,\n",
    "                heads=heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "                lstm_dim=lstm_dim,\n",
    "                window_size=window_size\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adjust the output dimension of the final linear layer to match the concatenated output\n",
    "        self.fc_out = nn.Linear(embed_size + lstm_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        lstm_hidden = None\n",
    "        states = None  # Initialize states as None\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x, states, lstm_hidden = block(x, x, x, mask, states, lstm_hidden) # Update x with the output from each block\n",
    "\n",
    "        print(f\"Shape of x before fc_out: {x.shape}\")\n",
    "        x = x.contiguous().view(-1, embed_size, lstm_dim) # Ensure correct reshaping\n",
    "        print(f\"Shape after reshaping for fc_out: {x.shape}\")\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "embed_size = 256  # Embedding size\n",
    "num_blocks = 6  # Number of transformer blocks\n",
    "heads = 8  # Number of attention heads\n",
    "state_dim = 64  # Dimension of the state in SSMLayer\n",
    "lstm_dim = 64  # Dimension of the hidden state in LSTM\n",
    "forward_expansion = 4  # Expansion factor in feed forward network\n",
    "max_length = 512  # Maximum length of the sequence\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "\n",
    "# Create MAMBA model instance\n",
    "mamba_model = MAMBA(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_blocks=num_blocks,\n",
    "    heads=heads,\n",
    "    lstm_dim=lstm_dim,\n",
    "    forward_expansion=forward_expansion,\n",
    "    max_length=max_length,\n",
    "    dropout=dropout,\n",
    "    window_size=25  \n",
    ")\n",
    "\n",
    "# Define Device for Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = AdamW(mamba_model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    mamba_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "\n",
    "        # Create dummy mask when no attention_mask is given\n",
    "        mask = attention_mask.unsqueeze(1).repeat(1, heads, 1) if attention_mask is not None else None\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mamba_model(input_ids, mask)\n",
    "\n",
    "        # Flatten outputs and input ids\n",
    "        outputs_flattened = outputs.view(-1, outputs.size(-1))\n",
    "        input_ids_flattened = input_ids.view(-1)\n",
    "\n",
    "        # Shift input_ids along the sequence axis to create labels, and remove the last token\n",
    "        labels = torch.roll(input_ids_flattened, -1)\n",
    "        labels = labels[:-1]\n",
    "        outputs_flattened = outputs_flattened[:-1]\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = criterion(outputs_flattened, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 30522\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4096x256 and 32x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 291\u001b[0m\n\u001b[0;32m    288\u001b[0m mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Flatten outputs and input ids\u001b[39;00m\n\u001b[0;32m    294\u001b[0m outputs_flattened \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 239\u001b[0m, in \u001b[0;36mMAMBA.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    236\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m--> 239\u001b[0m     x, states, lstm_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(N, seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_dim)\n\u001b[0;32m    242\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 185\u001b[0m, in \u001b[0;36mModifiedTransformerBlock.forward\u001b[1;34m(self, value, key, query, mask, states, lstm_hidden)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, key, query, mask, states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lstm_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 185\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# Apply layer normalization and dropout to the sum of attention and query (Residual connection)\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention \u001b[38;5;241m+\u001b[39m query))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 59\u001b[0m, in \u001b[0;36mSparseAttention.forward\u001b[1;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m feature_size \u001b[38;5;241m==\u001b[39m head_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature size must be divisible by heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Reshape values, keys, and queries for multi-head attention\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, head_dim)\n\u001b[0;32m     60\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys(keys)\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, head_dim)\n\u001b[0;32m     61\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries(query)\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, head_dim)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x256 and 32x32)"
     ]
    }
   ],
   "source": [
    "#MAMBA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def get_valid_indices(sequence_length, window_size):\n",
    "    all_indices = list(range(window_size * 2 + 1))\n",
    "    valid_indices = []\n",
    "    for idx in all_indices:\n",
    "        if idx < sequence_length and idx >= 0:\n",
    "            valid_indices.append(idx)\n",
    "    return torch.tensor(valid_indices, dtype=torch.long)\n",
    "\n",
    "# Tokenizer and Dataset - same as your working code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "# Get the vocab size of the tokenizer\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, window_size):\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, query_len, feature_size = query.size()\n",
    "        key_len = keys.size(1)\n",
    "        head_dim = feature_size // self.heads\n",
    "        assert feature_size == head_dim * self.heads, \"Feature size must be divisible by heads\"\n",
    "\n",
    "        # Reshape values, keys, and queries for multi-head attention\n",
    "        values = self.values(values).view(N, -1, self.heads, head_dim)\n",
    "        keys = self.keys(keys).view(N, -1, self.heads, head_dim)\n",
    "        queries = self.queries(query).view(N, -1, self.heads, head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention = self.calculate_sparse_attention(queries, keys, self.window_size)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # [N, 1, 1, key_len]\n",
    "            mask = mask.expand(N, self.heads, query_len, key_len)\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "\n",
    "        # Apply softmax and calculate the output\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, -1, self.heads * self.head_dim)\n",
    "\n",
    "        # Before the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def calculate_sparse_attention(self, queries, keys, window_size):\n",
    "        N, query_len, heads, head_dim = queries.size()\n",
    "        _, seq_len, _, _ = keys.size()\n",
    "        attention_scores = torch.zeros((N, heads, query_len, seq_len), device=queries.device)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(seq_len, i + window_size + 1)\n",
    "\n",
    "            keys_window = keys[:, start:end, :, :]\n",
    "            queries_window = queries[:, i:i+1, :, :].repeat(1, end - start, 1, 1)\n",
    "\n",
    "            for h in range(heads):\n",
    "                keys_head = keys_window[:, :, h, :]\n",
    "                queries_head = queries_window[:, :, h, :]\n",
    "\n",
    "                keys_head_reshaped = keys_head.reshape(N, end - start, head_dim)\n",
    "                queries_head_reshaped = queries_head.reshape(N, end - start, head_dim)\n",
    "\n",
    "                scores_head = torch.bmm(queries_head_reshaped, keys_head_reshaped.transpose(1, 2))\n",
    "\n",
    "                valid_indices = get_valid_indices(end - start, window_size)\n",
    "                broadcast_mask = torch.zeros_like(scores_head, device=queries.device)\n",
    "                broadcast_mask[..., valid_indices] = 1\n",
    "                summed_scores_head = scores_head.sum(dim=2)\n",
    "                summed_mask = broadcast_mask.sum(dim=2)\n",
    "                attention_scores[:, h, i, start:end] = summed_scores_head / summed_mask.clamp(min=1.)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "class SSMLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SSMLayer, self).__init__()\n",
    "        self.state_update = nn.Linear(input_dim, input_dim)\n",
    "        self.state_process = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        if states is None:\n",
    "            states = torch.zeros_like(x)\n",
    "        print(f\"states: {states.shape}\")\n",
    "        new_states = self.state_update(x) + states\n",
    "        print(f\"new_states: {new_states.shape}\")\n",
    "        processed_states = self.state_process(new_states)\n",
    "        print(f\"processed_states: {processed_states.shape}\")\n",
    "        return processed_states, new_states\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class ModifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, lstm_dim, window_size):\n",
    "        super(ModifiedTransformerBlock, self).__init__()\n",
    "        self.attention = SparseAttention(embed_size, heads, window_size)\n",
    "        self.norm1 = nn.LayerNorm(embed_size )\n",
    "        self.norm2 = nn.LayerNorm(embed_size + lstm_dim)\n",
    "        self.ssm_layer = SSMLayer(embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_dim, batch_first=True)\n",
    "        self.projection = nn.Linear(lstm_dim, embed_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size + embed_size, forward_expansion * (embed_size + embed_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * (embed_size + embed_size), embed_size + embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask, states=None, lstm_hidden=None):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        if lstm_hidden is not None:\n",
    "            x = torch.cat((query, lstm_hidden[0][:, :, :embed_size]), dim=-1)\n",
    "        else:\n",
    "            x = query\n",
    "        x = self.dropout(self.norm1(x))\n",
    "        processed_states, new_states = self.ssm_layer(x, states)\n",
    "        if lstm_hidden is None:\n",
    "            lstm_hidden = (torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device),\n",
    "                           torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device))\n",
    "        lstm_output, new_lstm_hidden = self.lstm(x)\n",
    "        lstm_output = self.projection(lstm_output)  # Project LSTM output to the desired space\n",
    "        x = torch.cat((x, lstm_output), dim=-1)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(self.norm2(x))\n",
    "\n",
    "        return x, new_states, new_lstm_hidden\n",
    "'''    \n",
    "class ModifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, lstm_dim, window_size):\n",
    "        super(ModifiedTransformerBlock, self).__init__()\n",
    "        self.attention = SparseAttention(embed_size, heads, window_size)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Layer norm before concatenation\n",
    "        self.norm2 = nn.LayerNorm(embed_size * 2)        \n",
    "        self.ssm_layer = SSMLayer(embed_size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_dim, batch_first=True)\n",
    "        self.projection = nn.Linear(lstm_dim, embed_size)  # Project LSTM output to embed_size\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size + embed_size, forward_expansion * (embed_size + embed_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * (embed_size + embed_size), embed_size + embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask, states=None, lstm_hidden=None):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Apply layer normalization and dropout to the sum of attention and query (Residual connection)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "        # Process with SSMLayer\n",
    "        processed_states, new_states = self.ssm_layer(x, states)\n",
    "\n",
    "        # LSTM processing\n",
    "        if lstm_hidden is None:\n",
    "            lstm_hidden = (torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device),\n",
    "                           torch.zeros(1, value.size(0), self.lstm.hidden_size, device=value.device))\n",
    "        lstm_output, new_lstm_hidden = self.lstm(x)\n",
    "        lstm_output = self.projection(lstm_output)\n",
    "\n",
    "        # Concatenate LSTM output with the processed attention output\n",
    "        x = torch.cat((x, lstm_output), dim=-1)\n",
    "\n",
    "        # Final feed-forward network\n",
    "        x = self.ffn(self.dropout(self.norm2(x)))\n",
    "\n",
    "        return x, new_states, new_lstm_hidden\n",
    "\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_blocks, heads, lstm_dim, forward_expansion, max_length, dropout, window_size):\n",
    "        super(MAMBA, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, self.embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            ModifiedTransformerBlock(\n",
    "                embed_size=self.embed_size,\n",
    "                heads=heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "                lstm_dim=self.lstm_dim,\n",
    "                window_size=window_size\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(self.embed_size + self.lstm_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        x = self.word_embedding(x) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        lstm_hidden = None\n",
    "        states = None\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x, states, lstm_hidden = block(x, x, x, mask, states, lstm_hidden)\n",
    "\n",
    "        x = x.view(N, seq_length, self.embed_size + self.lstm_dim)\n",
    "        x = x.mean(dim=1)\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "embed_size = 256  # Embedding size\n",
    "num_blocks = 6  # Number of transformer blocks\n",
    "heads = 8  # Number of attention heads\n",
    "state_dim = 64  # Dimension of the state in SSMLayer\n",
    "lstm_dim = 64  # Dimension of the hidden state in LSTM\n",
    "forward_expansion = 4  # Expansion factor in feed forward network\n",
    "max_length = 512  # Maximum length of the sequence\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "\n",
    "# Create MAMBA model instance\n",
    "mamba_model = MAMBA(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_blocks=num_blocks,\n",
    "    heads=heads,\n",
    "    lstm_dim=lstm_dim,\n",
    "    forward_expansion=forward_expansion,\n",
    "    max_length=max_length,\n",
    "    dropout=dropout,\n",
    "    window_size=25  \n",
    ")\n",
    "\n",
    "# Define Device for Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = AdamW(mamba_model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    mamba_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "\n",
    "        # Create dummy mask when no attention_mask is given\n",
    "        mask = attention_mask if attention_mask is not None else None\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mamba_model(input_ids, mask)\n",
    "\n",
    "        # Flatten outputs and input ids\n",
    "        outputs_flattened = outputs.view(-1, outputs.size(-1))\n",
    "        input_ids_flattened = input_ids.view(-1)\n",
    "\n",
    "        # Shift input_ids along the sequence axis to create labels, and remove the last token\n",
    "        labels = torch.roll(input_ids_flattened, -1)\n",
    "        labels = labels[:-1]\n",
    "        outputs_flattened = outputs_flattened[:-1]\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = criterion(outputs_flattened, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the tokenizer for your custom model\n",
    "\n",
    "custom_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # or another tokenizer if not BERT\n",
    "\n",
    "\n",
    "\n",
    "# Initialize your custom model (already fine-tuned with DPO)\n",
    "\n",
    "custom_model = LanguageModelTransformer(...)\n",
    "\n",
    "custom_model.load_state_dict(torch.load('path_to_fine_tuned_model.pt'))  # Load your fine-tuned model\n",
    "\n",
    "\n",
    "\n",
    "# Set up RAG components\n",
    "\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-base\")\n",
    "\n",
    "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-base\", index_name=\"custom\", passages_path=\"path_to_your_passages_file\")\n",
    "\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-base\", retriever=rag_retriever)\n",
    "\n",
    "\n",
    "\n",
    "def unified_inference_pipeline(input_text):\n",
    "\n",
    "    # Generate initial response using your custom model\n",
    "\n",
    "    inputs = custom_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = custom_model(**inputs)\n",
    "\n",
    "    response = custom_tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Augment response using RAG\n",
    "\n",
    "    rag_inputs = rag_tokenizer(response, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = rag_model.generate(rag_inputs[\"input_ids\"])\n",
    "\n",
    "    augmented_response = rag_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    return augmented_response\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "input_text = \"Your input query here\"\n",
    "\n",
    "augmented_response = unified_inference_pipeline(input_text)\n",
    "\n",
    "print(augmented_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH ROUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch Router:\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class RoutingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, response_dim, num_models):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(response_dim * num_models, num_models)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, responses):\n",
    "\n",
    "        # Concatenate responses and pass through the fully connected layer\n",
    "\n",
    "        combined_responses = torch.cat(responses, dim=1)\n",
    "\n",
    "        weights = self.fc(combined_responses)\n",
    "\n",
    "        return F.softmax(weights, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class IntegratedModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model1, model2, model3, response_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model1 = model1  # Transformer with DPO\n",
    "\n",
    "        self.model2 = model2  # RAG\n",
    "\n",
    "        self.model3 = model3  # MAMBA\n",
    "\n",
    "        self.routing_layer = RoutingLayer(response_dim, 3)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, input_prompt):\n",
    "\n",
    "        # Assuming each model returns a response vector of the same dimension\n",
    "\n",
    "        response1 = self.model1(input_prompt)\n",
    "\n",
    "        response2 = self.model2(input_prompt)\n",
    "\n",
    "        response3 = self.model3(input_prompt)\n",
    "\n",
    "\n",
    "\n",
    "        # Routing\n",
    "\n",
    "        weights = self.routing_layer([response1, response2, response3])\n",
    "\n",
    "\n",
    "\n",
    "        # Weighted sum of responses\n",
    "\n",
    "        final_response = weights[:, 0, None] * response1 + weights[:, 1, None] * response2 + weights[:, 2, None] * response3\n",
    "\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# Assuming IntegratedModel is already defined and instantiated as integrated_model\n",
    "\n",
    "# Assuming a DataLoader named 'data_loader' providing input prompts and corresponding target tokens\n",
    "\n",
    "\n",
    "\n",
    "# Define AdamW optimizer for the integrated model\n",
    "\n",
    "optimizer = optim.AdamW(integrated_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function - Cross-Entropy Loss for language modeling\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # tokenizer.pad_token_id for the padding token ID\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 5  # Number of training epochs\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    integrated_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        input_prompts, target_tokens = batch['input_prompts'], batch['target_tokens']\n",
    "\n",
    "        \n",
    "\n",
    "        # Forward pass through integrated model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = integrated_model(input_prompts)\n",
    "\n",
    "        \n",
    "\n",
    "        # Reshape for calculating loss (Cross-Entropy expects 2D input)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "\n",
    "        loss = criterion(outputs, target_tokens)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        # Backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save model state if needed\n",
    "\n",
    "torch.save(integrated_model.state_dict(), 'path_to_save_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWITCH TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MoESwitchTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_experts, response_dim, expert_params):\n",
    "\n",
    "        super(MoESwitchTransformer, self).__init__()\n",
    "\n",
    "        self.experts = nn.ModuleList([IntegratedModel(**expert_params) for _ in range(num_experts)])\n",
    "\n",
    "        self.routing_layer = RoutingLayer(response_dim, num_experts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_prompt):\n",
    "\n",
    "        expert_responses = [expert(input_prompt) for expert in self.experts]\n",
    "\n",
    "        # Combine responses for routing\n",
    "\n",
    "        combined_responses = torch.stack(expert_responses, dim=1)\n",
    "\n",
    "        # Routing: Shape of weights will be [batch_size, num_experts]\n",
    "\n",
    "        weights = self.routing_layer(combined_responses)\n",
    "\n",
    "        # Weighted sum of expert responses\n",
    "\n",
    "        final_response = torch.sum(weights.unsqueeze(2) * combined_responses, dim=1)\n",
    "\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n",
    "# Define the MoE model\n",
    "\n",
    "num_experts = 5  # Number of experts\n",
    "\n",
    "response_dim = 512  # Dimension of response (assumed for illustration)\n",
    "\n",
    "expert_params = {'model1': model_dpo, 'model2': model_rag, 'model3': model_mamba, 'response_dim': response_dim}\n",
    "\n",
    "\n",
    "\n",
    "moe_switch_transformer = MoESwitchTransformer(num_experts, response_dim, expert_params)\n",
    "\n",
    "\n",
    "\n",
    "# Define an input prompt (for example purposes)\n",
    "\n",
    "input_prompt = torch.rand(1, 512)  # Replace with actual input\n",
    "\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "output = moe_switch_transformer(input_prompt)\n",
    "\n",
    "print(output.shape)  # Output shape will depend on the response_dim\n",
    "\n",
    "\n",
    "\n",
    "# Training the MoE Switch Transformer\n",
    "\n",
    "# Define optimizer and loss function as before\n",
    "\n",
    "optimizer = torch.optim.AdamW(moe_switch_transformer.parameters(), lr=5e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Define according to your specific task\n",
    "\n",
    "\n",
    "\n",
    "# Assume 'data_loader' is provided\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    moe_switch_transformer.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, targets = batch  # Assuming batch contains inputs and targets\n",
    "\n",
    "        outputs = moe_switch_transformer(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
