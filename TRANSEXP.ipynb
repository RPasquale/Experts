{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TER v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        # Initializes the TransformerExperienceReplay class.\n",
    "        self.capacity = capacity  # Maximum number of experiences the buffer can hold.\n",
    "        self.buffer = []  # List to store experiences.\n",
    "        self.position = 0  # Tracks the next position to insert an experience.\n",
    "        # Initializes a transformer model with specified input dimensions and architecture parameters.\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, num_encoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # Adds an experience to the replay buffer.\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            # If buffer is not full, append a placeholder (None).\n",
    "            self.buffer.append(None)\n",
    "        # Inserts the new experience at the current position.\n",
    "        self.buffer[self.position] = experience\n",
    "        # Updates the position, wrapping around if it reaches the capacity.\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        # Samples experiences from the buffer based on transformer attention mechanism.\n",
    "        if len(self.buffer) < batch_size:\n",
    "            # Returns an empty list if there aren't enough experiences for a batch.\n",
    "            return []\n",
    "\n",
    "        # Prepares the current state as the query for the transformer.\n",
    "        # Adds extra dimensions to fit the transformer's input requirements.\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        # Stacks all non-None experiences to create a tensor for keys and values.\n",
    "        keys_values = torch.stack([exp for exp in self.buffer if exp is not None])\n",
    "        # Adds an extra dimension to the keys and values tensor.\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "\n",
    "        # Passes the query, keys, and values through the transformer model.\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        # Applies softmax to the attention output to get probabilities for sampling.\n",
    "        attention_weights = torch.softmax(attention_output, dim=0)\n",
    "\n",
    "        # Samples indices based on the attention weights.\n",
    "        sampled_indices = torch.multinomial(attention_weights.view(-1), batch_size, replacement=True)\n",
    "        # Retrieves experiences corresponding to the sampled indices.\n",
    "        sampled_experiences = [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "        # Returns the sampled experiences.\n",
    "        return sampled_experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of experiences currently in the buffer.\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        # Counts the number of trainable parameters in the transformer model.\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "# Prints the number of trainable parameters in the transformer model.\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TER v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim, nhead=4, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=nhead, \n",
    "                                          num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        keys_values = torch.stack([torch.cat(exp) for exp in self.buffer if exp is not None])\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TER v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        keys_values = torch.stack([torch.cat(exp) for exp in self.buffer if exp is not None])\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from gym) (1.26.2)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage\\nstate_size = env.observation_space.shape[0]  # Assuming \\'env\\' is your Gym environment\\naction_size = env.action_space.n\\ndqn_agent = DQN(state_size, action_size)\\n\\n\\n\\n\\n# Initialize the Gym environment\\nenv = gym.make(\\'CartPole-v1\\')\\n\\n# Parameters\\ninput_dim = env.observation_space.shape[0] + 2  # state dim + action + reward\\ncapacity = 1000\\n\\n# Initialize TER\\nter = TransformerExperienceReplay(capacity, input_dim)\\n\\n\\n\\nfor episode in range(num_episodes):\\n    state = env.reset()\\n    total_reward = 0\\n\\n    while True:\\n        action = agent.select_action(state)\\n        next_state, reward, done, _ = env.step(action)\\n\\n        # Store experience in TER\\n        experience = (state, action, reward, next_state)\\n        ter.push(experience)\\n\\n        # Sample from TER for learning\\n        if len(ter) > batch_size:\\n            experiences = ter.sample(batch_size, current_state=torch.tensor(state))\\n            agent.learn(experiences)\\n\\n        state = next_state\\n        total_reward += reward\\n\\n        if done:\\n            break\\n\\n    print(f\"Episode {episode}: Total Reward: {total_reward}\")\\n\\nenv.close()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        keys_values = torch.stack([torch.cat(exp) for exp in self.buffer if exp is not None])\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize TER and DQN\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m ter \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerExperienceReplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQN(state_size, action_size)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(state, epsilon):\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.__init__\u001b[1;34m(self, capacity, input_dim)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Standard transformer configuration can be set here\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mnum_encoder_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_decoder_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:99\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, custom_encoder, custom_decoder, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m custom_encoder\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     encoder_layer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_norm_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     encoder_norm \u001b[38;5;241m=\u001b[39m LayerNorm(d_model, eps\u001b[38;5;241m=\u001b[39mlayer_norm_eps, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:553\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[0;32m    551\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:991\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m embed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[1;32m--> 991\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim must be divisible by num_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        keys_values = torch.stack([torch.cat(exp) for exp in self.buffer if exp is not None])\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "input_dim = state_size + 2  # state dim + action + reward\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()  # Explore: select a random action\n",
    "    else:\n",
    "        q_values = dqn_agent.get_qvals(state)\n",
    "        return np.argmax(q_values)  # Exploit: select the action with max value\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Adjust reward for terminal states\n",
    "        reward_adj = reward if not done or total_reward == 499 else -100\n",
    "\n",
    "        # Store experience in TER\n",
    "        experience = (state, action, reward_adj, next_state, done)\n",
    "        ter.push(experience)\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=torch.tensor(state, dtype=torch.float32))\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n",
      "Step output: (array([ 0.04639766, -0.2045838 ,  0.01733584,  0.32029274], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04230599, -0.00971296,  0.0237417 ,  0.03312691], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04211173,  0.18506062,  0.02440424, -0.25197172], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04581294,  0.37982574,  0.0193648 , -0.53685826], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.05340946,  0.18443695,  0.00862764, -0.23813714], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.05709819,  0.3794346 ,  0.00386489, -0.52808625], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.06468689,  0.18425848, -0.00669683, -0.23418798], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.06837206,  0.37947547, -0.01138059, -0.5289757 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.07596157,  0.18451545, -0.02196011, -0.23990048], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.07965188,  0.37994412, -0.02675812, -0.53942853], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.08725076,  0.18520834, -0.03754669, -0.2552954 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.09095492,  0.3808457 , -0.0426526 , -0.55958074], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.09857184,  0.5765396 , -0.05384421, -0.8653906 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.11010263,  0.38219017, -0.07115202, -0.5901116 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.11774644,  0.57823247, -0.08295426, -0.90433204], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.12931108,  0.774374  , -0.10104089, -1.2218932 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.14479856,  0.58068854, -0.12547876, -0.9625023 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.15641233,  0.38745567, -0.14472881, -0.71172583], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.16416144,  0.58425355, -0.15896332, -1.0462383 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.17584652,  0.78108716, -0.17988808, -1.3843048 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.19146825,  0.97793806, -0.20757419, -1.7274188 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.21102703,  1.1747406 , -0.24212256, -2.0768752 ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 0: Total Reward: 122.0\n",
      "Step output: (array([ 0.03167889, -0.24114214, -0.02367608,  0.28813997], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02685605, -0.4359186 , -0.01791328,  0.57326263], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01813768, -0.63078487, -0.00644803,  0.8602489 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00552198, -0.8258184 ,  0.01075695,  1.1508975 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.01099439, -1.0210791 ,  0.0337749 ,  1.446934  ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03141597, -1.2165997 ,  0.06271359,  1.7499757 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.05574796, -1.0222435 ,  0.0977131 ,  1.4774401 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.07619283, -1.2184135 ,  0.12726189,  1.7989734 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.1005611 , -1.4147089 ,  0.16324137,  2.1283493 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.12885527, -1.2215416 ,  0.20580836,  1.8902309 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.15328611, -1.4182192 ,  0.24361297,  2.2391026 ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 1: Total Reward: 111.0\n",
      "Step output: (array([ 0.04886094, -0.19168179, -0.02626049,  0.28482333], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04502731, -0.38641956, -0.02056402,  0.56910956], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03729892, -0.58124715, -0.00918183,  0.85524374], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02567397, -0.7762428 ,  0.00792305,  1.1450255 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01014912, -0.9714673 ,  0.03082356,  1.4401824 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.00928023, -1.1669551 ,  0.05962721,  1.7423354 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03261933, -0.9725603 ,  0.09447391,  1.4687815 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.05207054, -1.1687028 ,  0.12384955,  1.7894174 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.07544459, -0.9751691 ,  0.1596379 ,  1.5376592 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.09494797, -0.7822879 ,  0.19039108,  1.2987504 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.11059373, -0.9792473 ,  0.21636608,  1.6444885 ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 2: Total Reward: 111.0\n",
      "Step output: (array([-0.03465162,  0.16281168,  0.04159474, -0.30415314], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03139539,  0.35731694,  0.03551167, -0.5834334 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02424905,  0.16171595,  0.02384301, -0.27977863], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02101473, -0.03373787,  0.01824743,  0.02032794], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02168949,  0.1611177 ,  0.01865399, -0.26654232], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.01846713,  0.35596853,  0.01332314, -0.55328375], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.01134776,  0.5509009 ,  0.00225747, -0.8417394 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-3.2974282e-04,  3.5574818e-01, -1.4577319e-02, -5.4834741e-01],\n",
      "      dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00678522,  0.16083401, -0.02554427, -0.26029283], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.0100019 , -0.03391415, -0.03075012,  0.02422491], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00932362,  0.16163498, -0.03026563, -0.27799922], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01255632,  0.35717535, -0.03582561, -0.58007216], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01969982,  0.16257325, -0.04742705, -0.2988868 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02295129, -0.0318417 , -0.05340479, -0.02153063], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02231446, -0.22615872, -0.0538354 ,  0.2538358 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01779128, -0.03031105, -0.04875869, -0.05533006], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01718506,  0.16547489, -0.04986529, -0.3629893 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02049456, -0.02890418, -0.05712507, -0.08643746], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01991647, -0.22316274, -0.05885382,  0.18768944], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01545322, -0.41739544, -0.05510003,  0.461241  ], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 185\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 185\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    188\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     33\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[1;32m---> 34\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_keys_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(query, keys_values, keys_values)\n\u001b[0;32m     37\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m, in \u001b[0;36mTransformerExperienceReplay._prepare_keys_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_keys_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 47\u001b[0m     keys_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m)\n\u001b[0;32m     48\u001b[0m     keys_values \u001b[38;5;241m=\u001b[39m keys_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keys_values\n",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_keys_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 47\u001b[0m     keys_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;28;01mif\u001b[39;00m exp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m     48\u001b[0m     keys_values \u001b[38;5;241m=\u001b[39m keys_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keys_values\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        keys_values = torch.stack([torch.cat(exp) for exp in self.buffer if exp is not None])\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()  # Explore: select a random action\n",
    "    else:\n",
    "        q_values = dqn_agent.get_qvals(state)\n",
    "        return np.argmax(q_values)  # Exploit: select the action with max value\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        step_output = env.step(action)\n",
    "        print(\"Step output:\", step_output)  # Debugging print statement\n",
    "\n",
    "        try:\n",
    "            next_state, reward, done, _ , _= step_output\n",
    "        except ValueError as e:\n",
    "            print(\"Error unpacking step output:\", e)\n",
    "            print(\"Step output:\", step_output)\n",
    "            break\n",
    "\n",
    "        # Adjust reward for terminal states\n",
    "        reward_adj = reward if not done or total_reward == 499 else -100\n",
    "\n",
    "        # Store experience in TER\n",
    "        experience = (state, action, reward_adj, next_state, done)\n",
    "        ter.push(experience)\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=torch.tensor(state, dtype=torch.float32))\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n",
      "Step output: (array([-0.05043406, -0.23217718, -0.00655086,  0.29743278], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.0550776 , -0.42720515, -0.00060221,  0.5880425 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.0636217 , -0.23207475,  0.01115865,  0.29516992], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.0682632 , -0.03711365,  0.01706204,  0.00602704], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.06900547, -0.23247609,  0.01718258,  0.30404404], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.07365499, -0.42783865,  0.02326347,  0.6020961 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.08221176, -0.62327814,  0.03530538,  0.9020148 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.09467733, -0.8188602 ,  0.05334568,  1.2055826 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.11105453, -0.62446666,  0.07745734,  0.93008333], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.12354387, -0.4304708 ,  0.096059  ,  0.6627118 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.13215329, -0.23680727,  0.10931323,  0.40175363], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.13688943, -0.43329617,  0.11734831,  0.72680193], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14555535, -0.23997532,  0.13188435,  0.47323626], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.15035486, -0.04693846,  0.14134908,  0.22485718], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.15129362,  0.14591013,  0.14584622, -0.02011234], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14837542, -0.05096962,  0.14544398,  0.3147989 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14939481,  0.14181346,  0.15173995,  0.07128741], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14655855, -0.05512142,  0.1531657 ,  0.40773597], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14766097,  0.13753472,  0.16132042,  0.16698973], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.14491028,  0.33002403,  0.16466022, -0.07077257], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.1383098 ,  0.52244943,  0.16324475, -0.307315  ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.12786081,  0.7149146 ,  0.15709846, -0.5443934 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.11356252,  0.5179745 ,  0.1462106 , -0.20662393], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.10320303,  0.71073604,  0.14207812, -0.4498482 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.0889883 ,  0.9035927 ,  0.13308115, -0.69458777], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.07091645,  0.7069005 ,  0.1191894 , -0.36314774], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.05677845,  0.510304  ,  0.11192644, -0.03538645], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.04657236,  0.70365775,  0.11121871, -0.2907646 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03249921,  0.50714016,  0.10540342,  0.03482144], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02235641,  0.7006051 ,  0.10609984, -0.22283414], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.0083443 ,  0.8940632 ,  0.10164316, -0.48025462], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00953696,  1.0876144 ,  0.09203807, -0.73925143], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03128925,  1.2813531 ,  0.07725304, -1.0016085 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.05691631,  1.0852886 ,  0.05722087, -0.68569905], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.07862209,  0.889421  ,  0.04350689, -0.37556463], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.09641051,  0.69370896,  0.0359956 , -0.06948738], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.11028469,  0.88829684,  0.03460585, -0.35059974], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.12805063,  0.69270027,  0.02759386, -0.04720844], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.14190462,  0.8874159 ,  0.02664969, -0.33105913], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.15965295,  1.0821486 ,  0.02002851, -0.6152203 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.18129592,  0.8867526 ,  0.0077241 , -0.31629717], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.19903097,  1.0817636 ,  0.00139816, -0.6065342 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.22066624,  0.8866222 , -0.01073253, -0.3134112 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.23839869,  0.6916548 , -0.01700075, -0.02413222], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.25223178,  0.4967807 , -0.0174834 ,  0.26313868], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.2621674 ,  0.6921478 , -0.01222062, -0.03500703], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.27601036,  0.8874428 , -0.01292076, -0.3315205 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.2937592 ,  1.0827463 , -0.01955117, -0.6282498 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.31541413,  1.2781355 , -0.03211617, -0.92702544], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.34097683,  1.4736761 , -0.05065668, -1.2296256 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.37045038,  1.6694119 , -0.07524919, -1.5377393 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.4038386 ,  1.8653545 , -0.10600398, -1.8529239 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.4411457 ,  1.6715459 , -0.14306246, -1.5949494 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.47457662,  1.8680454 , -0.17496143, -1.9286048 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.5119375 ,  2.0645585 , -0.21353354, -2.2700505 ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 0: Total Reward: 155.0\n",
      "Step output: (array([-0.02599737, -0.22198229, -0.01069065,  0.31468752], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03043702, -0.0267097 , -0.0043969 ,  0.01865238], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03097121,  0.16847503, -0.00402385, -0.2754146 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02760171,  0.36365417, -0.00953214, -0.5693639 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02032863,  0.16866718, -0.02091942, -0.27969915], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.01695528,  0.3640812 , -0.02651341, -0.5789059 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.00967366,  0.16934067, -0.03809152, -0.2946918 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.00628685,  0.3649844 , -0.04398536, -0.5991407 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00101284,  0.56069326, -0.05596817, -0.9053476 ], dtype=float32), 1.0, False, False, {})\n",
      "Invalid state or next_state format: (array([-0.04969102, -0.03715178, -0.00668821,  0.00686753], dtype=float32), {}) [-0.05043406 -0.23217718 -0.00655086  0.29743278]\n",
      "Invalid state or next_state format: (array([-0.02545692, -0.02702276, -0.01120185,  0.02555984], dtype=float32), {}) [-0.02599737 -0.22198229 -0.01069065  0.31468752]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 213\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    216\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     33\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[0;32m     34\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_keys_values()\n\u001b[1;32m---> 36\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     39\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(attention_weights, batch_size, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    205\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[0;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        query = current_state.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "                \n",
    "                # Check if state and next_state are in correct format\n",
    "                if isinstance(state, np.ndarray) and isinstance(next_state, np.ndarray):\n",
    "                    state_tensor = torch.from_numpy(state).float()\n",
    "                    next_state_tensor = torch.from_numpy(next_state).float()\n",
    "                else:\n",
    "                    # Handle cases where state or next_state might not be a numpy array\n",
    "                    print(\"Invalid state or next_state format:\", state, next_state)\n",
    "                    continue  # Skip this experience\n",
    "\n",
    "                # Convert action, reward, and done to tensors\n",
    "                action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "                reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "                done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "                \n",
    "                exp_tensor = torch.cat((state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor))\n",
    "                processed_experiences.append(exp_tensor)\n",
    "\n",
    "        keys_values = torch.stack(processed_experiences)\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()  # Explore: select a random action\n",
    "    else:\n",
    "        # Ensure state is a numpy array before passing to get_qvals\n",
    "        state_array = np.array(state) if not isinstance(state, np.ndarray) else state\n",
    "        q_values = dqn_agent.get_qvals(state_array)\n",
    "        return np.argmax(q_values)  # Exploit: select the action with max value\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        step_output = env.step(action)\n",
    "        print(\"Step output:\", step_output)  # Debugging print statement\n",
    "\n",
    "        try:\n",
    "            next_state, reward, done, _ , _= step_output\n",
    "        except ValueError as e:\n",
    "            print(\"Error unpacking step output:\", e)\n",
    "            print(\"Step output:\", step_output)\n",
    "            break\n",
    "\n",
    "        # Adjust reward for terminal states\n",
    "        reward_adj = reward if not done or total_reward == 499 else -100\n",
    "\n",
    "        # Store experience in TER\n",
    "        experience = (state, action, reward_adj, next_state, done)\n",
    "        ter.push(experience)\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=torch.tensor(state, dtype=torch.float32))\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n",
      "Step output: (array([ 0.03040762,  0.1812452 , -0.02534003, -0.25116494], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03403252,  0.37671965, -0.03036333, -0.5517317 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04156691,  0.18203701, -0.04139796, -0.26876774], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04520765,  0.37772453, -0.04677331, -0.5742151 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.05276214,  0.57346994, -0.05825762, -0.8812584 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.06423154,  0.3791857 , -0.07588279, -0.6074445 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.07181525,  0.575282  , -0.08803167, -0.9230306 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.08332089,  0.3814526 , -0.10649229, -0.6592602 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.08332089  0.3814526  -0.10649229 -0.6592602 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.09094995,  0.5778828 , -0.11967749, -0.983486  ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.10250761,  0.7743872 , -0.13934721, -1.3112354 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.11799534,  0.9709713 , -0.16557193, -1.644089  ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.13741477,  1.1675992 , -0.1984537 , -1.9834533 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.16076675,  1.3641773 , -0.23812276, -2.330496  ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 0: Total Reward: 113.0\n",
      "Step output: (array([-0.04532805, -0.20312288, -0.00237108,  0.29189244], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.04939051, -0.39821094,  0.00346677,  0.5838266 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.04939051 -0.39821094  0.00346677  0.5838266 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.05735473, -0.20313774,  0.0151433 ,  0.2922378 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.05735473 -0.20313774  0.0151433   0.2922378 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.06141748, -0.00823494,  0.02098805,  0.00436907], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.06141748 -0.00823494  0.02098805  0.00436907] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.06158218,  0.18657982,  0.02107544, -0.28161868], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.06158218  0.18657982  0.02107544 -0.28161868] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.05785059,  0.38139492,  0.01544306, -0.5675807 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.05022269,  0.18605979,  0.00409145, -0.27007285], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.04650149,  0.3811231 , -0.00131001, -0.5614625 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03887903,  0.18601957, -0.01253926, -0.2691926 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.03515864,  0.38131818, -0.01792311, -0.56580395], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.02753228,  0.5766869 , -0.02923919, -0.8640791 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.01599854,  0.77219445, -0.04652077, -1.1658101 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-5.5464724e-04,  9.6789002e-01, -6.9836974e-02, -1.4727081e+00],\n",
      "      dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01880315,  1.1637928 , -0.09929114, -1.7863613 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04207901,  1.3598799 , -0.13501836, -2.1081867 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.06927661,  1.1663437 , -0.1771821 , -1.8600997 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.09260348,  0.97355324, -0.2143841 , -1.6272607 ], dtype=float32), 1.0, True, False, {})\n",
      "Episode 1: Total Reward: 117.0\n",
      "Step output: (array([ 0.01801359, -0.18810906,  0.03904482,  0.25901988], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01425141,  0.00643436,  0.04422521, -0.02109659], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.01425141  0.00643436  0.04422521 -0.02109659] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.0143801 ,  0.2008951 ,  0.04380328, -0.29950452], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([0.018398  , 0.00517704, 0.03781319, 0.00666503], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01850154,  0.19973686,  0.03794649, -0.27385166], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.01850154  0.19973686  0.03794649 -0.27385166] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.02249628,  0.39429742,  0.03246946, -0.5543288 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03038223,  0.19873494,  0.02138288, -0.25159538], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.03038223  0.19873494  0.02138288 -0.25159538] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.03435693,  0.39354512,  0.01635098, -0.53745776], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04222783,  0.19819716,  0.00560182, -0.239668  ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([0.04619177, 0.00299563, 0.00080846, 0.05477662], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04625168, -0.1921379 ,  0.00190399,  0.3477145 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([0.04240892, 0.00295691, 0.00885828, 0.05563259], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04246806, -0.19229092,  0.00997093,  0.35109717], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.04246806 -0.19229092  0.00997093  0.35109717] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([0.03862225, 0.00268782, 0.01699288, 0.06157498], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.038676  , -0.1926736 ,  0.01822438,  0.35957047], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03482253, -0.3880498 ,  0.02541579,  0.6579438 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02706153, -0.19329067,  0.03857466,  0.37337086], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02319572, -0.38893875,  0.04604208,  0.67796284], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01541694, -0.19448568,  0.05960134,  0.4001239 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.01152723, -0.3904002 ,  0.06760381,  0.7109863 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.01152723 -0.3904002   0.06760381  0.7109863 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.00371923, -0.19627635,  0.08182354,  0.44032604], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [ 0.00371923 -0.19627635  0.08182354  0.44032604] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.0002063 , -0.00240194,  0.09063006,  0.1745167 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.0002063  -0.00240194  0.09063006  0.1745167 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.00025434,  0.19131392,  0.09412039, -0.0882558 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00357194, -0.00502229,  0.09235528,  0.23257558], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.00347149, -0.2013342 ,  0.09700679,  0.55290234], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([-0.00055519, -0.00769877,  0.10806484,  0.2922901 ], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.00055519 -0.00769877  0.10806484  0.2922901 ] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([-0.00070917,  0.18572976,  0.11391064,  0.03554905], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [-0.00070917  0.18572976  0.11391064  0.03554905] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.00300543,  0.37904954,  0.11462162, -0.21913399], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([0.01058642, 0.1824916 , 0.11023894, 0.10739326], dtype=float32), 1.0, False, False, {})\n",
      "State before conversion: [0.01058642 0.1824916  0.11023894 0.10739326] Type: <class 'numpy.ndarray'>\n",
      "State array shape: (4,)\n",
      "Step output: (array([ 0.01423625,  0.37587532,  0.11238681, -0.14857686], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.02175376,  0.56922346,  0.10941527, -0.40379724], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.03313823,  0.37273365,  0.10133933, -0.07872102], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([0.0405929 , 0.176316  , 0.09976491, 0.24413672], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Current state is not a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 247\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 247\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    250\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m<\u001b[39m batch_size:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m---> 33\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_keys_values()\n\u001b[0;32m     36\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(query, keys_values, keys_values)\n",
      "Cell \u001b[1;32mIn[18], line 47\u001b[0m, in \u001b[0;36mTransformerExperienceReplay._prepare_query\u001b[1;34m(self, current_state)\u001b[0m\n\u001b[0;32m     45\u001b[0m     current_state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(current_state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent state is not a numpy array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Ensure current_state has the correct size\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state_tensor\u001b[38;5;241m.\u001b[39mnelement() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m128\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Current state is not a numpy array"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        # 'experience' is expected to be a tuple: (state, action, reward, next_state, target)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        # Convert state to tensor if it's a numpy array\n",
    "        if isinstance(current_state, np.ndarray):\n",
    "            current_state_tensor = torch.from_numpy(current_state).float()\n",
    "        else:\n",
    "            raise ValueError(\"Current state is not a numpy array\")\n",
    "\n",
    "        # Ensure current_state has the correct size\n",
    "        if current_state_tensor.nelement() == 128:\n",
    "            query = current_state_tensor.unsqueeze(0).unsqueeze(1)\n",
    "        else:\n",
    "            # Pad or truncate the current_state to have a size of 128\n",
    "            query = torch.nn.functional.pad(current_state_tensor, (0, 128 - current_state_tensor.nelement()), \"constant\", 0)\n",
    "            query = query.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "                \n",
    "                # Verify that state and next_state are numpy arrays\n",
    "                if not (isinstance(state, np.ndarray) and isinstance(next_state, np.ndarray)):\n",
    "                    print(\"Invalid state or next_state format:\", state, next_state)\n",
    "                    continue\n",
    "\n",
    "                state_tensor = torch.from_numpy(state).float()\n",
    "                next_state_tensor = torch.from_numpy(next_state).float()\n",
    "\n",
    "                # Convert action, reward, and done to tensors\n",
    "                action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "                reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "                done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "                \n",
    "                # Concatenate tensors to create a single tensor for the experience\n",
    "                exp_tensor = torch.cat((state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor))\n",
    "                \n",
    "                # Ensure exp_tensor has the correct size\n",
    "                if exp_tensor.nelement() == 128:\n",
    "                    processed_experiences.append(exp_tensor)\n",
    "                else:\n",
    "                    # Pad or truncate to have a size of 128\n",
    "                    padded_tensor = torch.nn.functional.pad(exp_tensor, (0, 128 - exp_tensor.nelement()), \"constant\", 0)\n",
    "                    processed_experiences.append(padded_tensor)\n",
    "\n",
    "        keys_values = torch.stack(processed_experiences)\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()  # Explore: select a random action\n",
    "    else:\n",
    "        # Debugging: Check the state's shape and type\n",
    "        print(\"State before conversion:\", state, \"Type:\", type(state))\n",
    "        \n",
    "        # Convert state to a NumPy array if it is not already\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            try:\n",
    "                state_array = np.asarray(state, dtype=np.float32)\n",
    "            except Exception as e:\n",
    "                print(\"Error converting state to NumPy array:\", e)\n",
    "                print(\"State:\", state)\n",
    "                return env.action_space.sample()  # Fallback action\n",
    "        else:\n",
    "            state_array = state\n",
    "\n",
    "        # Debugging: Check the state array's shape after conversion\n",
    "        print(\"State array shape:\", state_array.shape)\n",
    "\n",
    "        q_values = dqn_agent.get_qvals(state_array)\n",
    "        return np.argmax(q_values)  # Exploit: select the action with max value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        step_output = env.step(action)\n",
    "        print(\"Step output:\", step_output)  # Debugging print statement\n",
    "\n",
    "        try:\n",
    "            next_state, reward, done, _ , _= step_output\n",
    "        except ValueError as e:\n",
    "            print(\"Error unpacking step output:\", e)\n",
    "            print(\"Step output:\", step_output)\n",
    "            break\n",
    "\n",
    "        # Adjust reward for terminal states\n",
    "        reward_adj = reward if not done or total_reward == 499 else -100\n",
    "\n",
    "        # Store experience in TER\n",
    "        experience = (state, action, reward_adj, next_state, done)\n",
    "        ter.push(experience)\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=torch.tensor(state, dtype=torch.float32))\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 3D attn_mask is torch.Size([1, 1, 8]), but should be (4, 1, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 208\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 208\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    211\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[38], line 39\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     36\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[0;32m     37\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_keys_values()\n\u001b[1;32m---> 39\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     42\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(attention_weights, batch_size, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:5323\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5321\u001b[0m     correct_3d_size \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m num_heads, tgt_len, src_len)\n\u001b[0;32m   5322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_3d_size:\n\u001b[1;32m-> 5323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 3D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_3d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The shape of the 3D attn_mask is torch.Size([1, 1, 8]), but should be (4, 1, 1)."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = [None] * capacity\n",
    "        self.position = 0\n",
    "        # Standard transformer configuration can be set here\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        state_tensor = state.clone().detach()\n",
    "        next_state_tensor = next_state.clone().detach()\n",
    "\n",
    "        action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "        self.buffer[self.position] = (state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        # Pad or truncate to match the size of d_model\n",
    "        current_state_padded = torch.nn.functional.pad(current_state, (0, self.transformer.d_model - current_state.nelement()), \"constant\", 0)\n",
    "        query = current_state_padded.view(1, 1, self.transformer.d_model)  # Reshape to [1, 1, d_model]\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "\n",
    "                # Use the tensors directly as they are already in the correct format\n",
    "                state_tensor = state\n",
    "                next_state_tensor = next_state\n",
    "\n",
    "                # Convert action, reward, and done to tensors\n",
    "                action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "                reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "                done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "\n",
    "                # Concatenate tensors to create a single tensor for the experience\n",
    "                exp_tensor = torch.cat((state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor))\n",
    "\n",
    "                # Ensure exp_tensor has the correct size\n",
    "                if exp_tensor.nelement() < self.transformer.d_model:\n",
    "                    # Pad to match the size of d_model\n",
    "                    exp_tensor_padded = torch.nn.functional.pad(exp_tensor, (0, self.transformer.d_model - exp_tensor.nelement()), \"constant\", 0)\n",
    "                else:\n",
    "                    # Truncate to match the size of d_model\n",
    "                    exp_tensor_padded = exp_tensor[:self.transformer.d_model]\n",
    "\n",
    "                processed_experiences.append(exp_tensor_padded.view(1, 1, self.transformer.d_model))  # Reshape to [1, 1, d_model]\n",
    "\n",
    "        keys_values = torch.cat(processed_experiences, dim=0)  # Concatenate along the batch dimension\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        # Assuming 'state' is already a PyTorch tensor\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy() \n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    # Assuming state is already a tensor\n",
    "    q_values = dqn_agent.get_qvals(state.unsqueeze(0))  # Add batch dimension\n",
    "    return np.argmax(q_values) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    initial_state_info = env.reset()\n",
    "    state = initial_state_info[0] if isinstance(initial_state_info, tuple) else initial_state_info\n",
    "    state = torch.from_numpy(state).float()  # Convert initial state to tensor\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).float()  # Convert next state to tensor\n",
    "\n",
    "        # Adjust reward for terminal states and convert to tensor\n",
    "        reward_adj = torch.tensor([reward if not done or total_reward == 499 else -100], dtype=torch.float32)\n",
    "\n",
    "        ter.push((state, action, reward_adj, next_state, torch.tensor([done], dtype=torch.float32)))\n",
    "        state = next_state  # Update state to tensor\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=state)\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 3D attn_mask is torch.Size([1, 1, 8]), but should be (4, 1, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 205\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    208\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[42], line 40\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     37\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[0;32m     38\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_keys_values()\n\u001b[1;32m---> 40\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     43\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(attention_weights, batch_size, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:5323\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5321\u001b[0m     correct_3d_size \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m num_heads, tgt_len, src_len)\n\u001b[0;32m   5322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_3d_size:\n\u001b[1;32m-> 5323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 3D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_3d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The shape of the 3D attn_mask is torch.Size([1, 1, 8]), but should be (4, 1, 1)."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = [None] * capacity\n",
    "        self.position = 0\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "\n",
    "    def push(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        state_tensor = state.clone().detach()\n",
    "        next_state_tensor = next_state.clone().detach()\n",
    "\n",
    "        action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "        self.buffer[self.position] = (state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state, batch_size=1, seq_length=1):\n",
    "        current_state_flat = current_state.view(-1)\n",
    "        # Pad or truncate to match the size of d_model (input_dim)\n",
    "        if current_state_flat.nelement() < self.input_dim:\n",
    "            current_state_padded = torch.nn.functional.pad(current_state_flat, (0, self.input_dim - current_state_flat.nelement()), \"constant\", 0)\n",
    "        else:\n",
    "            current_state_padded = current_state_flat[:self.input_dim]\n",
    "\n",
    "        # Reshape to [batch_size, seq_length, feature_dim]\n",
    "        query = current_state_padded.view(batch_size, seq_length, -1)\n",
    "        return query\n",
    "\n",
    "    def _prepare_keys_values(self, batch_size=1, seq_length=1):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "                exp_tensor = torch.cat([s.view(-1) for s in [state, next_state, action, reward, done]])\n",
    "                # Pad or truncate to match the size of d_model (input_dim)\n",
    "                if exp_tensor.nelement() < self.input_dim:\n",
    "                    exp_tensor_padded = torch.nn.functional.pad(exp_tensor, (0, self.input_dim - exp_tensor.nelement()), \"constant\", 0)\n",
    "                else:\n",
    "                    exp_tensor_padded = exp_tensor[:self.input_dim]\n",
    "\n",
    "                processed_experiences.append(exp_tensor_padded.view(batch_size, seq_length, -1))\n",
    "\n",
    "        if not processed_experiences:\n",
    "            return torch.zeros((batch_size, seq_length, self.input_dim))\n",
    "\n",
    "        keys_values = torch.cat(processed_experiences, dim=0)\n",
    "        return keys_values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        # Assuming 'state' is already a PyTorch tensor\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy() \n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    # Assuming state is already a tensor\n",
    "    q_values = dqn_agent.get_qvals(state.unsqueeze(0))  # Add batch dimension\n",
    "    return np.argmax(q_values) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    initial_state_info = env.reset()\n",
    "    state = initial_state_info[0] if isinstance(initial_state_info, tuple) else initial_state_info\n",
    "    state = torch.from_numpy(state).float()  # Convert initial state to tensor\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).float()  # Convert next state to tensor\n",
    "\n",
    "        # Adjust reward for terminal states and convert to tensor\n",
    "        reward_adj = torch.tensor([reward if not done or total_reward == 499 else -100], dtype=torch.float32)\n",
    "\n",
    "        ter.push((state, action, reward_adj, next_state, torch.tensor([done], dtype=torch.float32)))\n",
    "        state = next_state  # Update state to tensor\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=state)\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the transformer: 2505216\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 194\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Sample from TER for learning\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 194\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    197\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[43], line 40\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     37\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[0;32m     38\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_keys_values()\n\u001b[1;32m---> 40\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     43\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(attention_weights, batch_size, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    205\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[0;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = [None] * capacity\n",
    "        self.position = 0\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "\n",
    "    def push(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        state_tensor = state.clone().detach()\n",
    "        next_state_tensor = next_state.clone().detach()\n",
    "\n",
    "        action_tensor = torch.tensor([action], dtype=torch.float32)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "        self.buffer[self.position] = (state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state, batch_size=1, seq_length=1):\n",
    "        # Flatten current_state and adjust its length to match d_model\n",
    "        current_state_flat = current_state.view(-1)\n",
    "        if current_state_flat.nelement() < self.input_dim:\n",
    "            current_state_padded = torch.nn.functional.pad(current_state_flat, (0, self.input_dim - current_state_flat.nelement()), \"constant\", 0)\n",
    "        else:\n",
    "            current_state_padded = current_state_flat[:self.input_dim]\n",
    "\n",
    "        # Reshape to [batch_size, seq_length, feature_dim]\n",
    "        query = current_state_padded.view(batch_size, seq_length, self.input_dim)\n",
    "        return query\n",
    "\n",
    "\n",
    "    def _prepare_keys_values(self, batch_size=1, seq_length=1):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "                exp_tensor = torch.cat([s.view(-1) for s in [state, next_state, action, reward, done]])\n",
    "                if exp_tensor.nelement() < self.input_dim:\n",
    "                    exp_tensor_padded = torch.nn.functional.pad(exp_tensor, (0, self.input_dim - exp_tensor.nelement()), \"constant\", 0)\n",
    "                else:\n",
    "                    exp_tensor_padded = exp_tensor[:self.input_dim]\n",
    "\n",
    "                exp_tensor_padded = exp_tensor_padded.view(batch_size, seq_length, self.input_dim)\n",
    "                processed_experiences.append(exp_tensor_padded)\n",
    "\n",
    "        if not processed_experiences:\n",
    "            return torch.zeros((batch_size, seq_length, self.input_dim))\n",
    "\n",
    "        keys_values = torch.cat(processed_experiences, dim=0)\n",
    "        return keys_values\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "replay = TransformerExperienceReplay(capacity=1000, input_dim=128)\n",
    "print(\"Number of parameters in the transformer:\", replay.count_parameters())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        # Assuming 'state' is already a PyTorch tensor\n",
    "        with torch.no_grad():\n",
    "            qvals = self.model(state)\n",
    "        return qvals.numpy() \n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values for current states\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute Q values for next states using target network\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)  # gamma is the discount factor\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Adjust input_dim to be divisible by the number of heads in the transformer (which is 4)\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)  # Increase input_dim to the next multiple of 4\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize TER and DQN with the adjusted input_dim\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    # Assuming state is already a tensor\n",
    "    q_values = dqn_agent.get_qvals(state.unsqueeze(0))  # Add batch dimension\n",
    "    return np.argmax(q_values) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    initial_state_info = env.reset()\n",
    "    state = initial_state_info[0] if isinstance(initial_state_info, tuple) else initial_state_info\n",
    "    state = torch.from_numpy(state).float()  # Convert initial state to tensor\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).float()  # Convert next state to tensor\n",
    "\n",
    "        # Adjust reward for terminal states and convert to tensor\n",
    "        reward_adj = torch.tensor([reward if not done or total_reward == 499 else -100], dtype=torch.float32)\n",
    "\n",
    "        ter.push((state, action, reward_adj, next_state, torch.tensor([done], dtype=torch.float32)))\n",
    "        state = next_state  # Update state to tensor\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "        # Sample from TER for learning\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=state)\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward = total_reward if total_reward == 500 else total_reward + 100\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Episode 0: Total Reward: 10.0\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Episode 1: Total Reward: 26.0\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Episode 2: Total Reward: 12.0\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n",
      "Step output: (array([ 0.04411922, -0.02007892,  0.10464764,  0.5665465 ], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 1 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state_info  \u001b[38;5;66;03m# Update the state with the new state info\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ter) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 169\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[43mter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mtrain_one_step(experiences)\n\u001b[0;32m    172\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[29], line 35\u001b[0m, in \u001b[0;36mTransformerExperienceReplay.sample\u001b[1;34m(self, batch_size, current_state)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     34\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query(current_state)\n\u001b[1;32m---> 35\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_keys_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(query, keys_values, keys_values)\n\u001b[0;32m     38\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 64\u001b[0m, in \u001b[0;36mTransformerExperienceReplay._prepare_keys_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m         reward_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     62\u001b[0m         done_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([done], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m         exp_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m         processed_experiences\u001b[38;5;241m.\u001b[39mappend(exp_tensor)\n\u001b[0;32m     67\u001b[0m keys_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(processed_experiences)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 1 and 2"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class TransformerExperienceReplay:\n",
    "    def __init__(self, capacity, input_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, \n",
    "                                          num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "    def push(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        # Convert state and next_state to numpy arrays if they are not already\n",
    "        state_array = np.array(state, dtype=np.float32) if not isinstance(state, np.ndarray) else state\n",
    "        next_state_array = np.array(next_state, dtype=np.float32) if not isinstance(next_state, np.ndarray) else next_state\n",
    "        \n",
    "        # Ensure buffer size does not exceed capacity\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "\n",
    "        self.buffer[self.position] = (state_array, action, reward, next_state_array, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, current_state):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "\n",
    "        query = self._prepare_query(current_state)\n",
    "        keys_values = self._prepare_keys_values()\n",
    "\n",
    "        attention_output = self.transformer(query, keys_values, keys_values)\n",
    "        attention_weights = torch.softmax(attention_output.squeeze(), dim=0)\n",
    "\n",
    "        sampled_indices = torch.multinomial(attention_weights, batch_size, replacement=True)\n",
    "        return [self.buffer[i] for i in sampled_indices]\n",
    "\n",
    "    def _prepare_query(self, current_state):\n",
    "        current_state_tensor = torch.from_numpy(current_state).float()\n",
    "        query = current_state_tensor.unsqueeze(0).unsqueeze(1)\n",
    "        return query\n",
    "\n",
    "\n",
    "\n",
    "    def _prepare_keys_values(self):\n",
    "        processed_experiences = []\n",
    "        for exp in self.buffer:\n",
    "            if exp is not None:\n",
    "                state, action, reward, next_state, done = exp\n",
    "                if not (isinstance(state, np.ndarray) and isinstance(next_state, np.ndarray)):\n",
    "                    continue\n",
    "\n",
    "                state_tensor = torch.from_numpy(state).float()\n",
    "                next_state_tensor = torch.from_numpy(next_state).float()\n",
    "                action_tensor = torch.tensor([action], dtype=torch.float32).unsqueeze(0)\n",
    "                reward_tensor = torch.tensor([reward], dtype=torch.float32).unsqueeze(0)\n",
    "                done_tensor = torch.tensor([done], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                exp_tensor = torch.cat((state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor), dim=0)\n",
    "                processed_experiences.append(exp_tensor)\n",
    "\n",
    "        keys_values = torch.stack(processed_experiences)\n",
    "        keys_values = keys_values.unsqueeze(1)\n",
    "        return keys_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.transformer.parameters() if p.requires_grad)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1, l2, l3, l4 = state_size, 24, 24, action_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(l1, l2), nn.ReLU(), nn.Linear(l2, l3), nn.ReLU(), nn.Linear(l3, l4))\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.model(state).numpy()\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return torch.max(self.model2(state)).item()\n",
    "\n",
    "    def train_one_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        Q_current = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        Q_next = self.model2(next_states).detach().max(1)[0]\n",
    "        Q_target = rewards + gamma * Q_next * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(Q_current, Q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "def select_action(state_info, epsilon):\n",
    "    # Assuming state_info is a tuple where the first element is the actual state\n",
    "    state = state_info[0] if isinstance(state_info, tuple) else state_info\n",
    "\n",
    "    # Convert state to a NumPy array if it is not already\n",
    "    try:\n",
    "        state_array = np.asarray(state, dtype=np.float32)\n",
    "    except ValueError as e:\n",
    "        print(\"Error converting state to NumPy array:\", e)\n",
    "        print(\"State:\", state)\n",
    "        return env.action_space.sample()  # Fallback action\n",
    "\n",
    "    q_values = dqn_agent.get_qvals(state_array)\n",
    "    return np.argmax(q_values) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "input_dim = state_size + 2\n",
    "if input_dim % 4 != 0:\n",
    "    input_dim += (4 - input_dim % 4)\n",
    "\n",
    "capacity = 1000\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "\n",
    "ter = TransformerExperienceReplay(capacity, input_dim)\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state_info, reward, done, _, _ = env.step(action)\n",
    "        print(\"Step output:\", step_output)\n",
    "        # Extract the actual next state from next_state_info\n",
    "        next_state = next_state_info[0] if isinstance(next_state_info, tuple) else next_state_info\n",
    "        reward_adj = reward if not done or total_reward == 499 else -100\n",
    "\n",
    "        ter.push((state[0], action, reward_adj, next_state, done))  # Ensure you're passing the actual state\n",
    "        state = next_state_info  # Update the state with the new state info\n",
    "\n",
    "        if len(ter) >= batch_size:\n",
    "            experiences = ter.sample(batch_size, current_state=state)\n",
    "            dqn_agent.train_one_step(experiences)\n",
    "\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            dqn_agent.update_target()\n",
    "\n",
    "    print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
